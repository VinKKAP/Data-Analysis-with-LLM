{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VinKKAP/Data-Analysis-with-LLM/blob/main/Experiment_mit_Qwen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cC-ECJCiC5QZ",
    "outputId": "91d09fcf-f61c-4e1d-c16b-9041a4712286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9jpnfJuDCI3",
    "outputId": "e001fbce-c6f9-488c-ec83-19bbd9fd3dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Data-Analysis-with-LLM'...\n",
      "remote: Enumerating objects: 236, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 236 (delta 6), reused 9 (delta 2), pack-reused 215 (from 1)\u001b[K\n",
      "Receiving objects: 100% (236/236), 57.47 MiB | 16.00 MiB/s, done.\n",
      "Resolving deltas: 100% (75/75), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/VinKKAP/Data-Analysis-with-LLM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvAX46fhDG-7",
    "outputId": "c11c41cf-5de5-4019-c910-6e37d02e2fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from -r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (2.13.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from -r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from -r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 3)) (3.20.3)\n",
      "Requirement already satisfied: scikit_learn in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from -r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from -r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 5)) (1.10.1)\n",
      "Requirement already satisfied: simpletransformers in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from -r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.64.3)\n",
      "Requirement already satisfied: torch in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from -r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 7)) (2.5.1+cu124)\n",
      "Requirement already satisfied: transformers in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from -r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 8)) (4.46.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (14.0.2)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (0.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from pandas->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from pandas->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from scikit_learn->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from scikit_learn->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: regex in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (2023.10.3)\n",
      "Requirement already satisfied: seqeval in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (2.18.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.20.3)\n",
      "Requirement already satisfied: wandb>=0.10.32 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.18.5)\n",
      "Requirement already satisfied: streamlit in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (1.40.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from torch->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 7)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from torch->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from torch->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 7)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from torch->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 7)) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from torch->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from transformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 8)) (0.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from aiohttp->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from aiohttp->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from aiohttp->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from aiohttp->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from aiohttp->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (3.1.37)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (3.10.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (68.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from jinja2->torch->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 7)) (2.1.3)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (4.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (10.2.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (13.3.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.10.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (6.3.3)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (2.1.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (2.2.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (4.0.7)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (4.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers->-r C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\Paper\\requirements.txt (line 6)) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /content/Data-Analysis-with-LLM/Paper/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9LEqB1GEjdj",
    "outputId": "b3e2808f-ff3e-4952-c153-e44b06a7824f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, tokenizers, transformers\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.31.0\n",
      "    Uninstalling transformers-4.31.0:\n",
      "      Successfully uninstalled transformers-4.31.0\n",
      "Successfully installed scikit-learn-1.5.2 tokenizers-0.20.3 transformers-4.46.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8NFu3pzFj4m",
    "outputId": "c2b922b2-a824-4801-d433-d93b81917f72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1\n",
      "Uninstalling torch-2.5.1:\n",
      "  Successfully uninstalled torch-2.5.1\n",
      "Found existing installation: transformers 4.46.3\n",
      "Uninstalling transformers-4.46.3:\n",
      "  Successfully uninstalled transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\kappv\\Desktop\\Data-Analysis-with-LLM-4\\env\\Lib\\site-packages\\~orch'.\n",
      "You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kappv\\desktop\\data-analysis-with-llm-4\\env\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Using cached torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Installing collected packages: torch, transformers\n",
      "Successfully installed torch-2.5.1 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y torch transformers\n",
    "%pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kappv\\anaconda3\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6tAwW3wBh7PK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache() # Removed the extra indentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  4 10:28:12 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.03                 Driver Version: 566.03         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1070      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   38C    P8             16W /  230W |     862MiB /   8192MiB |     14%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1828    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A      3740    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      4616    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      4628    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A      5852    C+G   ...tionsPlus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A      6788    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A      8732    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9832    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A      9856    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A      9896    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     10264    C+G   ...mpt_builder\\LogiAiPromptBuilder.exe      N/A      |\n",
      "|    0   N/A  N/A     11972    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     12200    C+G   ...on\\131.0.2903.70\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     12580    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12824    C+G   ...on\\131.0.2903.70\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     13548    C+G   ...ns\\Software\\Current\\LogiOverlay.exe      N/A      |\n",
      "|    0   N/A  N/A     14608    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     15744    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     17660    C+G   ... Stream\\100.0.2.0\\GoogleDriveFS.exe      N/A      |\n",
      "|    0   N/A  N/A     18976    C+G   ...45.0_x64__zpdnekdrzrea0\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A     19292    C+G   ....0_x64__kzh8wxbdkxb8p\\DCv2\\DCv2.exe      N/A      |\n",
      "|    0   N/A  N/A     22132    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8KZCUokFKwdt",
    "outputId": "505833b0-c5c7-4d64-8bd2-57fc0277398f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_recall_fscore_support\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m     24\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m userdata\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on Aug 12, 2023\n",
    "\n",
    "@author: immanueltrummer\n",
    "'''\n",
    "from multiprocessing import set_start_method\n",
    "try:\n",
    "    set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "import argparse\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "from google.colab import userdata\n",
    "userdata.get('HF_TOKEN')\n",
    "\n",
    "def add_type(row):\n",
    "    \"\"\" Enrich column name by adding column type.\n",
    "\n",
    "    Args:\n",
    "        row: describes correlation between two columns.\n",
    "\n",
    "    Returns:\n",
    "        row with enriched column names.\n",
    "    \"\"\"\n",
    "    row['column1'] = row['column1'] + ' ' + row['type1']\n",
    "    row['column2'] = row['column2'] + ' ' + row['type2']\n",
    "    return row\n",
    "\n",
    "def def_split(data, test_ratio, seed):\n",
    "    \"\"\" Split data into training and test set.\n",
    "\n",
    "    With this approach, different column pairs from the\n",
    "    same data set may appear in training and test set.\n",
    "\n",
    "    Args:\n",
    "        data: a pandas dataframe containing all data.\n",
    "        test_ratio: ratio of test cases after split.\n",
    "        seed: random seed for deterministic results.\n",
    "\n",
    "    Returns:\n",
    "        a tuple containing training, then test data.\n",
    "    \"\"\"\n",
    "    print('Data sets in training and test may overlap')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "      data[['column1', 'column2', 'type1', 'type2']], data['label'],\n",
    "      test_size=test_ratio, random_state=seed)\n",
    "    train = pd.concat([x_train, y_train], axis=1)\n",
    "    test = pd.concat([x_test, y_test], axis=1)\n",
    "    print(f'train shape: {train.shape}')\n",
    "    print(f'test shape: {test.shape}')\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def ds_split(data, test_ratio):\n",
    "    \"\"\" Split column pairs into training and test samples.\n",
    "\n",
    "    With this method, training and test set contain columns\n",
    "    of disjunct data sets, making prediction a bit harder.\n",
    "\n",
    "    Args:\n",
    "        data: a pandas dataframe containing all data.\n",
    "        test_ratio: ratio of test cases after splitting.\n",
    "\n",
    "    Returns:\n",
    "        a tuple containing training, then test set.\n",
    "    \"\"\"\n",
    "    print('Separating training and test sets by data')\n",
    "    counts = data['dataid'].value_counts()\n",
    "    print(f'Counts: {counts}')\n",
    "    print(f'Count.index: {counts.index}')\n",
    "    print(f'Count.index.values: {counts.index.values}')\n",
    "    print(f'counts.shape: {counts.shape}')\n",
    "    print(f'counts.iloc[0]: {counts.iloc[0]}')\n",
    "    nr_vals = len(counts)\n",
    "    nr_test_ds = int(nr_vals * test_ratio)\n",
    "    print(f'Nr. test data sets: {nr_test_ds}')\n",
    "    ds_ids = counts.index.values.tolist()\n",
    "    print(type(ds_ids))\n",
    "    print(ds_ids)\n",
    "    test_ds = rand.sample(ds_ids, nr_test_ds)\n",
    "    print(f'TestDS: {test_ds}')\n",
    "\n",
    "    def is_test(row):\n",
    "        if row['dataid'] in test_ds:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    data['istest'] = data.apply(is_test, axis=1)\n",
    "    train = data[data['istest'] == False]\n",
    "    test = data[data['istest'] == True]\n",
    "    print(f'train.shape: {train.shape}')\n",
    "    print(f'test.shape: {test.shape}')\n",
    "    print(train)\n",
    "    print(test)\n",
    "    return train[\n",
    "        ['column1', 'column2', 'type1', 'type2', 'label']], test[\n",
    "            ['column1', 'column2', 'type1', 'type2', 'label']]\n",
    "\n",
    "\n",
    "def baseline(col_pairs):\n",
    "    \"\"\" A simple baseline predicting correlation via Jaccard similarity.\n",
    "\n",
    "    Args:\n",
    "        col_pairs: list of tuples with column names.\n",
    "\n",
    "    Returns:\n",
    "        list of predictions (1 for correlation, 0 for no correlation).\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for cp in col_pairs:\n",
    "        c1 = cp[0]\n",
    "        c2= cp[1]\n",
    "        s1 = set(c1.split())\n",
    "        s2 = set(c2.split())\n",
    "        ns1 = len(s1)\n",
    "        ns2 = len(s2)\n",
    "        ni = len(set.intersection(s1, s2))\n",
    "        # calculate Jaccard coefficient\n",
    "        jac = ni / (ns1 + ns2 - ni)\n",
    "        # predict correlation if similar\n",
    "        if jac > 0.5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# log all metrics into summary for data subset\n",
    "def log_metrics(\n",
    "        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
    "        test_ratio, sub_test, test_name, lb, ub, pred_method,\n",
    "        out_path, training_time):\n",
    "    \"\"\" Predicts using baseline or model, writes metrics to file.\n",
    "\n",
    "    Args:\n",
    "        coeff: predict correlation according to this coefficient.\n",
    "        min_v1: lower bound on coefficient value for correlation.\n",
    "        max_v2: upper bound on p-value to be considered correlated.\n",
    "        mod_type: base type of language model used for prediction.\n",
    "        mod_name: precise name of language model used for prediction.\n",
    "        scenario: how training and test data relate to each other.\n",
    "        test_ratio: ratio of column pairs used for testing (not training).\n",
    "        sub_test: data frame with test cases, possibly a subset.\n",
    "        test_name: write this test name into result file.\n",
    "        lb: lower bound on a test-specific metric constraining test cases.\n",
    "        ub: upper bound on test-specific metric, constraining test cases.\n",
    "        pred_method: whether to use language model or simple baseline.\n",
    "        out_path: path to result output file (results are appended).\n",
    "        training_time: time taken to train the model.\n",
    "    \"\"\"\n",
    "    sub_test.columns = [\n",
    "        'text_a', 'text_b', 'type1', 'type2', 'labels', 'length', 'nrtokens']\n",
    "    # print out a sample for later analysis\n",
    "    print(f'Sample for test {test_name}:')\n",
    "    sample = sub_test.sample(frac=0.1)\n",
    "    print(sample)\n",
    "    # predict correlation via baseline or model\n",
    "    sub_test = sub_test[['text_a', 'text_b', 'labels']]\n",
    "    samples = []\n",
    "    for _, r in sub_test.iterrows():\n",
    "        samples.append([r['text_a'], r['text_b']])\n",
    "    s_time = time.time()\n",
    "    if pred_method == 0:\n",
    "        preds = baseline(samples)\n",
    "    else:\n",
    "        preds = model.predict(samples)[0]\n",
    "    # log various performance metrics\n",
    "    t_time = time.time() - s_time\n",
    "    nr_samples = len(sub_test.index)\n",
    "    t_per_s = float(t_time) / nr_samples\n",
    "    f1 = metrics.f1_score(sub_test['labels'], preds)\n",
    "    pre = metrics.precision_score(sub_test['labels'], preds)\n",
    "    rec = metrics.recall_score(sub_test['labels'], preds)\n",
    "    acc = metrics.accuracy_score(sub_test['labels'], preds)\n",
    "    mcc = metrics.matthews_corrcoef(sub_test['labels'], preds)\n",
    "    # also log to local file\n",
    "    with open(out_path, 'a+') as file:\n",
    "        file.write(f'{coeff},{min_v1},{max_v2},\"{mod_type}\",' \\\n",
    "                f'\"{mod_name}\",\"{scenario}\",{test_ratio},' \\\n",
    "                f'\"{test_name}\",{pred_method},{lb},{ub},' \\\n",
    "                f'{f1},{pre},{rec},{acc},{mcc},{t_per_s},' \\\n",
    "                f'{training_time}\\n')\n",
    "\n",
    "\n",
    "def names_length(row):\n",
    "    \"\"\" Calculate combined length of column names.\n",
    "\n",
    "    Args:\n",
    "        row: contains information on one column pair.\n",
    "\n",
    "    Returns:\n",
    "        combined length of column names (in characters).\n",
    "    \"\"\"\n",
    "    return len(row['text_a']) + len(row['text_b'])\n",
    "\n",
    "def names_tokens(row):\n",
    "    \"\"\" Calculates number of tokens (separated by spaces).\n",
    "\n",
    "    Attention: this is not the number of tokens as calculated\n",
    "    by the tokenizer of the language model but an approximation.\n",
    "\n",
    "    Args:\n",
    "        row: contains information on one column pair.\n",
    "\n",
    "    Returns:\n",
    "        number of space-separated substrings in both column names.\n",
    "    \"\"\"\n",
    "    return row['text_a'].count(' ') + row['text_b'].count(' ')\n",
    "\n",
    "\n",
    "def run_experiment(src_path, coeff, min_v1, max_v2, mod_type, mod_name, scenario, test_ratio, use_types, out_path):\n",
    "    # print parameters\n",
    "    print(f'Coefficients: {coeff}')\n",
    "    print(f'Minimal value 1: {min_v1}')\n",
    "    print(f'Maximal value 2: {max_v2}')\n",
    "    print(f'Model type: {mod_type}')\n",
    "    print(f'Model name: {mod_name}')\n",
    "    print(f'Scenario: {scenario}')\n",
    "    print(f'Test ratio: {test_ratio}')\n",
    "\n",
    "    # initialize for deterministic results\n",
    "    seed = 42\n",
    "    rand.seed(seed)\n",
    "\n",
    "    # load data\n",
    "    data = pd.read_csv(src_path, sep=',')\n",
    "    data = data.sample(frac=1, random_state=seed)\n",
    "    data.columns = [\n",
    "        'dataid', 'datapath', 'nrrows', 'nrvals1', 'nrvals2',\n",
    "        'type1', 'type2', 'column1', 'column2', 'method',\n",
    "        'coefficient', 'pvalue', 'time']\n",
    "\n",
    "    # enrich column names if activated\n",
    "    if use_types:\n",
    "        data = data.apply(add_type, axis=1)\n",
    "\n",
    "    # Initialize the tokenizer and model from the pre-trained model name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(mod_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(mod_name, num_labels=2)\n",
    "\n",
    "    # Set a padding token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "         tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Update the model configuration to include the pad_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Check if GPU is available and set the device accordingly\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Move the model to the GPU if available\n",
    "    model.to(device)\n",
    "\n",
    "    # filter data\n",
    "    data = data[data['method'] == coeff]\n",
    "    nr_total = len(data.index)\n",
    "    print(f'Nr. samples: {nr_total}')\n",
    "    print('Sample from filtered data:')\n",
    "    print(data.head())\n",
    "\n",
    "    # label data\n",
    "    def coefficient_label(row):\n",
    "        \"\"\" Label column pair as correlated or uncorrelated.\n",
    "\n",
    "        Args:\n",
    "            row: describes correlation between column pair.\n",
    "\n",
    "        Returns:\n",
    "            1 if correlated, 0 if not correlated.\n",
    "        \"\"\"\n",
    "        if abs(row['coefficient']) >= min_v1 and abs(row['pvalue']) <= max_v2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    data['label'] = data.apply(coefficient_label, axis=1)\n",
    "\n",
    "    # split into test and training\n",
    "    if scenario == 'defsep':\n",
    "        train, test = def_split(data, test_ratio, seed)\n",
    "    elif scenario == 'datasep':\n",
    "        train, test = ds_split(data, test_ratio)\n",
    "    else:\n",
    "        raise ValueError(f'Undefined scenario: {scenario}')\n",
    "\n",
    "    train.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels']\n",
    "    test.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels']\n",
    "    print(train.head())\n",
    "    print(test.head())\n",
    "\n",
    "    # prepare dataset for transformers\n",
    "    train_encodings = tokenizer(train['text_a'].tolist(), train['text_b'].tolist(), truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test['text_a'].tolist(), test['text_b'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    train_dataset = Dataset(train_encodings, train['labels'].tolist())\n",
    "    test_dataset = Dataset(test_encodings, test['labels'].tolist())\n",
    "\n",
    "    # prepare training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=2,  # Reduce batch size\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=8,  # Maintain effective batch size\n",
    "        fp16=True,  # Enable mixed-precision training\n",
    "        warmup_steps=100,  # Reduce warmup steps\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=50,  # Reduce logging frequency\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        optim=\"adamw_torch\"  # Memory-efficient optimizer\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    s_time = time.time()\n",
    "    trainer.train()\n",
    "    training_time = time.time() - s_time\n",
    "\n",
    "    test['length'] = test.apply(names_length, axis=1)\n",
    "    test['nrtokens'] = test.apply(names_tokens, axis=1)\n",
    "\n",
    "    # Initialize result file\n",
    "    with open(out_path, 'w') as file:\n",
    "        file.write(\n",
    "            'coefficient,min_v1,max_v2,mod_type,mod_name,scenario,test_ratio,'\n",
    "            'test_name,pred_method,lb,ub,f1,precision,recall,accuracy,mcc,'\n",
    "            'prediction_time,training_time\\n')\n",
    "\n",
    "    # use simple baseline and model for prediction\n",
    "    for m in [0, 1]:\n",
    "        # use entire test set (redundant - for verification)\n",
    "        test_name = f'{m}-final'\n",
    "        log_metrics(\n",
    "            coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
    "            test_ratio, test, test_name, 0, 'inf', m, out_path, training_time)\n",
    "\n",
    "        # test for data types\n",
    "        for type1 in ['object', 'float64', 'int64', 'bool']:\n",
    "            for type2 in ['object', 'float64', 'int64', 'bool']:\n",
    "                sub_test = test.query(f'type1==\"{type1}\" and type2==\"{type2}\"')\n",
    "                if sub_test.shape[0]:\n",
    "                    test_name = f'Types{type1}-{type2}'\n",
    "                    log_metrics(\n",
    "                        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
    "                        test_ratio, sub_test, test_name, -1, -1, m,\n",
    "                        out_path, training_time)\n",
    "\n",
    "        # test for different subsets\n",
    "        for q in [(0, 0.25), (0.25, 0.5), (0.5, 1)]:\n",
    "            qlb = q[0]\n",
    "            qub = q[1]\n",
    "            # column name length\n",
    "            lb = test['length'].quantile(qlb)\n",
    "            ub = test['length'].quantile(qub)\n",
    "            sub_test = test[(test['length'] >= lb) & (test['length'] <= ub)]\n",
    "            test_name = f'L{m}-{qlb}-{qub}'\n",
    "            log_metrics(\n",
    "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
    "                test_ratio, sub_test, test_name, lb, ub, m, out_path, training_time)\n",
    "            # number of tokens in column names\n",
    "            lb = test['nrtokens'].quantile(qlb)\n",
    "            ub = test['nrtokens'].quantile(qub)\n",
    "            sub_test = test[(test['nrtokens'] >= lb) & (test['nrtokens'] <= ub)]\n",
    "            test_name = f'N{m}-{qlb}-{qub}'\n",
    "            log_metrics(\n",
    "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
    "                test_ratio, sub_test, test_name, lb, ub, m, out_path, training_time)\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='binary')\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Example usage in a Jupyter Notebook or Google Colab\n",
    "args = {\n",
    "    \"src_path\": \"/content/drive/My Drive/Colab Notebooks/Liter/correlations/correlationdata.csv\",\n",
    "    \"coeff\": \"pearson\",\n",
    "    \"min_v1\": 0.9,\n",
    "    \"max_v2\": 0.05,\n",
    "    \"mod_type\": \"Qwen\",\n",
    "    \"mod_name\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    \"scenario\": \"defsep\",\n",
    "    \"test_ratio\": 0.2,\n",
    "    \"use_types\": 1,\n",
    "    \"out_path\": \"/content/drive/My Drive/Colab Notebooks/Liter/correlations/models\"\n",
    "}\n",
    "\n",
    "run_experiment(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0z9nWDIdMdPQ",
    "outputId": "837754b5-dd5e-435f-ea2e-38aa3ad81e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.1.0\n",
      "accelerate==1.1.1\n",
      "aiohappyeyeballs==2.4.4\n",
      "aiohttp==3.11.9\n",
      "aiosignal==1.3.1\n",
      "altair==5.5.0\n",
      "asttokens==3.0.0\n",
      "attrs==24.2.0\n",
      "blinker==1.9.0\n",
      "cachetools==5.5.0\n",
      "certifi==2024.8.30\n",
      "charset-normalizer==3.4.0\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "comm==0.2.2\n",
      "datasets==3.1.0\n",
      "debugpy==1.8.9\n",
      "decorator==5.1.1\n",
      "dill==0.3.8\n",
      "docker-pycreds==0.4.0\n",
      "executing==2.1.0\n",
      "filelock==3.16.1\n",
      "frozenlist==1.5.0\n",
      "fsspec==2024.9.0\n",
      "gitdb==4.0.11\n",
      "GitPython==3.1.43\n",
      "grpcio==1.68.1\n",
      "huggingface-hub==0.26.3\n",
      "idna==3.10\n",
      "ipykernel==6.29.5\n",
      "ipython==8.30.0\n",
      "jedi==0.19.2\n",
      "Jinja2==3.1.4\n",
      "joblib==1.4.2\n",
      "jsonschema==4.23.0\n",
      "jsonschema-specifications==2024.10.1\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.7.2\n",
      "Markdown==3.7\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==3.0.2\n",
      "matplotlib-inline==0.1.7\n",
      "mdurl==0.1.2\n",
      "mpmath==1.3.0\n",
      "multidict==6.1.0\n",
      "multiprocess==0.70.16\n",
      "narwhals==1.15.2\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.4.2\n",
      "numpy==2.1.3\n",
      "packaging==24.2\n",
      "pandas==2.2.3\n",
      "parso==0.8.4\n",
      "pillow==11.0.0\n",
      "platformdirs==4.3.6\n",
      "prompt_toolkit==3.0.48\n",
      "propcache==0.2.1\n",
      "protobuf==5.29.0\n",
      "psutil==6.1.0\n",
      "pure_eval==0.2.3\n",
      "pyarrow==18.1.0\n",
      "pydeck==0.9.1\n",
      "Pygments==2.18.0\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.0.1\n",
      "pytz==2024.2\n",
      "pywin32==308\n",
      "PyYAML==6.0.2\n",
      "pyzmq==26.2.0\n",
      "referencing==0.35.1\n",
      "regex==2024.11.6\n",
      "requests==2.32.3\n",
      "rich==13.9.4\n",
      "rpds-py==0.22.1\n",
      "safetensors==0.4.5\n",
      "scikit-learn==1.5.2\n",
      "scipy==1.14.1\n",
      "sentencepiece==0.2.0\n",
      "sentry-sdk==2.19.0\n",
      "seqeval==1.2.2\n",
      "setproctitle==1.3.4\n",
      "simpletransformers==0.70.1\n",
      "six==1.16.0\n",
      "smmap==5.0.1\n",
      "stack-data==0.6.3\n",
      "streamlit==1.40.2\n",
      "sympy==1.13.1\n",
      "tenacity==9.0.0\n",
      "tensorboard==2.18.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorboardX==2.6.2.2\n",
      "threadpoolctl==3.5.0\n",
      "tokenizers==0.20.3\n",
      "toml==0.10.2\n",
      "torch==2.5.1\n",
      "torchaudio==2.5.1\n",
      "torchvision==0.20.1\n",
      "tornado==6.4.2\n",
      "tqdm==4.67.1\n",
      "traitlets==5.14.3\n",
      "transformers==4.46.3\n",
      "typing_extensions==4.12.2\n",
      "tzdata==2024.2\n",
      "urllib3==2.2.3\n",
      "wandb==0.18.7\n",
      "watchdog==6.0.0\n",
      "wcwidth==0.2.13\n",
      "Werkzeug==3.1.3\n",
      "xxhash==3.5.0\n",
      "yarl==1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMDtAY9WuHCHnY6rsyN13J8",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
