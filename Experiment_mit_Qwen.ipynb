{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhGchwsRNKl3dtFRYKslJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinKKAP/Data-Analysis-with-LLM/blob/main/Experiment_mit_Qwen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC-ECJCiC5QZ",
        "outputId": "79d01fb5-8192-4813-d267-b38791a6721e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")  # Assuming device 0\n",
        "else:\n",
        "    print(\"CUDA is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMEslf08Phwn",
        "outputId": "abf3406b-6793-4e11-ca27-21f73b0b9f52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!\n",
            "Number of CUDA devices: 1\n",
            "Current CUDA device: 0\n",
            "Device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5z72Ft90EY-e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VinKKAP/Data-Analysis-with-LLM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9jpnfJuDCI3",
        "outputId": "915ee6b5-c398-4bac-f648-74608d371b24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Data-Analysis-with-LLM'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 233 (delta 4), reused 9 (delta 2), pack-reused 215 (from 1)\u001b[K\n",
            "Receiving objects: 100% (233/233), 57.46 MiB | 16.31 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/Data-Analysis-with-LLM/Paper/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvAX46fhDG-7",
        "outputId": "a840109e-5613-4370-a088-21ed001fc6f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1))\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 3)) (4.25.5)\n",
            "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 5)) (1.13.1)\n",
            "Collecting simpletransformers (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6))\n",
            "  Downloading simpletransformers-0.70.1-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 8)) (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (4.66.6)\n",
            "Collecting xxhash (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2024.9.11)\n",
            "Collecting seqeval (from simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6))\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2.17.1)\n",
            "Collecting tensorboardx (from simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.20.3)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.18.7)\n",
            "Collecting streamlit (from simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6))\n",
            "  Downloading streamlit-1.40.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 8)) (0.4.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2024.8.30)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (4.3.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (75.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (3.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (5.5.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (11.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6))\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6))\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (6.3.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.68.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (4.0.11)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.1.2)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simpletransformers-0.70.1-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.40.2-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=add7dc4007921dfe34ecfa1a68bf0b81043cf0d6eb1a1bfeb2350315c3199106\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: xxhash, watchdog, tensorboardx, fsspec, dill, pydeck, multiprocess, seqeval, streamlit, datasets, simpletransformers\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 pydeck-0.9.1 seqeval-1.2.2 simpletransformers-0.70.1 streamlit-1.40.2 tensorboardx-2.6.2.2 watchdog-6.0.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip uninstall fsspec"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lThzf1TdOU2o",
        "outputId": "f683f7a4-e162-425b-88d2-778f680384c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: fsspec 2024.9.0\n",
            "Uninstalling fsspec-2024.9.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/fsspec-2024.9.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/fsspec/*\n",
            "Proceed (Y/n)? \u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/uninstall.py\", line 106, in run\n",
            "    uninstall_pathset = req.uninstall(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 722, in uninstall\n",
            "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 364, in remove\n",
            "    if auto_confirm or self._allowed_to_proceed(verbose):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 404, in _allowed_to_proceed\n",
            "    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 235, in ask\n",
            "    response = input(message)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 383, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 306, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 396, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 365, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.10/tokenize.py\", line 323, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9LEqB1GEjdj",
        "outputId": "e869c102-8643-4a69-b429-e01c92da1827"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
            "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn, tokenizers, transformers\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.31.0\n",
            "    Uninstalling transformers-4.31.0:\n",
            "      Successfully uninstalled transformers-4.31.0\n",
            "Successfully installed scikit-learn-1.5.2 tokenizers-0.20.3 transformers-4.46.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python colab_kernel_launcher.py <coeff> <min_v1> <max_v2> <mod_type> <mod_name> <scenario> <test_ratio> <use_types> <out_path>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMKd2D0eFWI1",
        "outputId": "29422f7f-bdf2-4165-b40c-a42d9f88a643"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `<'\n",
            "/bin/bash: -c: line 1: `python colab_kernel_launcher.py <coeff> <min_v1> <max_v2> <mod_type> <mod_name> <scenario> <test_ratio> <use_types> <out_path>'\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip uninstall -y torch transformers\n",
        "!pip install torch transformers\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I8NFu3pzFj4m",
        "outputId": "26e62695-0edc-4dd7-d97c-c03595f609f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.1\n",
            "Uninstalling torch-2.0.1:\n",
            "  Successfully uninstalled torch-2.0.1\n",
            "Found existing installation: transformers 4.46.3\n",
            "Uninstalling transformers-4.46.3:\n",
            "  Successfully uninstalled transformers-4.46.3\n",
            "Collecting torch\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.0.0\n",
            "    Uninstalling triton-2.0.0:\n",
            "      Successfully uninstalled triton-2.0.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 transformers-4.46.3 triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "transformers"
                ]
              },
              "id": "9ef2e94c7bca45ea86851a2b2d2fe140"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Aug 12, 2023\n",
        "\n",
        "@author: immanueltrummer\n",
        "'''\n",
        "from multiprocessing import set_start_method\n",
        "try:\n",
        "    set_start_method('spawn')\n",
        "except RuntimeError:\n",
        "    pass\n",
        "\n",
        "import argparse\n",
        "import sklearn.metrics as metrics\n",
        "import pandas as pd\n",
        "import random as rand\n",
        "import time\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from simpletransformers.classification import ClassificationArgs, ClassificationModel\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "\n",
        "\n",
        "def add_type(row):\n",
        "    \"\"\" Enrich column name by adding column type.\n",
        "\n",
        "    Args:\n",
        "        row: describes correlation between two columns.\n",
        "\n",
        "    Returns:\n",
        "        row with enriched column names.\n",
        "    \"\"\"\n",
        "    row['column1'] = row['column1'] + ' ' + row['type1']\n",
        "    row['column2'] = row['column2'] + ' ' + row['type2']\n",
        "    return row\n",
        "\n",
        "\n",
        "def def_split(data, test_ratio, seed):\n",
        "    \"\"\" Split data into training and test set.\n",
        "\n",
        "    With this approach, different column pairs from the\n",
        "    same data set may appear in training and test set.\n",
        "\n",
        "    Args:\n",
        "        data: a pandas dataframe containing all data.\n",
        "        test_ratio: ratio of test cases after split.\n",
        "        seed: random seed for deterministic results.\n",
        "\n",
        "    Returns:\n",
        "        a tuple containing training, then test data.\n",
        "    \"\"\"\n",
        "    print('Data sets in training and test may overlap')\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "      data[['column1', 'column2', 'type1', 'type2']], data['label'],\n",
        "      test_size=test_ratio, random_state=seed)\n",
        "    train = pd.concat([x_train, y_train], axis=1)\n",
        "    test = pd.concat([x_test, y_test], axis=1)\n",
        "    print(f'train shape: {train.shape}')\n",
        "    print(f'test shape: {test.shape}')\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def ds_split(data, test_ratio):\n",
        "    \"\"\" Split column pairs into training and test samples.\n",
        "\n",
        "    With this method, training and test set contain columns\n",
        "    of disjunct data sets, making prediction a bit harder.\n",
        "\n",
        "    Args:\n",
        "        data: a pandas dataframe containing all data.\n",
        "        test_ratio: ratio of test cases after splitting.\n",
        "\n",
        "    Returns:\n",
        "        a tuple containing training, then test set.\n",
        "    \"\"\"\n",
        "    print('Separating training and test sets by data')\n",
        "    counts = data['dataid'].value_counts()\n",
        "    print(f'Counts: {counts}')\n",
        "    print(f'Count.index: {counts.index}')\n",
        "    print(f'Count.index.values: {counts.index.values}')\n",
        "    print(f'counts.shape: {counts.shape}')\n",
        "    print(f'counts.iloc[0]: {counts.iloc[0]}')\n",
        "    nr_vals = len(counts)\n",
        "    nr_test_ds = int(nr_vals * test_ratio)\n",
        "    print(f'Nr. test data sets: {nr_test_ds}')\n",
        "    ds_ids = counts.index.values.tolist()\n",
        "    print(type(ds_ids))\n",
        "    print(ds_ids)\n",
        "    test_ds = rand.sample(ds_ids, nr_test_ds)\n",
        "    print(f'TestDS: {test_ds}')\n",
        "\n",
        "    def is_test(row):\n",
        "        if row['dataid'] in test_ds:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    data['istest'] = data.apply(is_test, axis=1)\n",
        "    train = data[data['istest'] == False]\n",
        "    test = data[data['istest'] == True]\n",
        "    print(f'train.shape: {train.shape}')\n",
        "    print(f'test.shape: {test.shape}')\n",
        "    print(train)\n",
        "    print(test)\n",
        "    return train[\n",
        "        ['column1', 'column2', 'type1', 'type2', 'label']], test[\n",
        "            ['column1', 'column2', 'type1', 'type2', 'label']]\n",
        "\n",
        "\n",
        "def baseline(col_pairs):\n",
        "    \"\"\" A simple baseline predicting correlation via Jaccard similarity.\n",
        "\n",
        "    Args:\n",
        "        col_pairs: list of tuples with column names.\n",
        "\n",
        "    Returns:\n",
        "        list of predictions (1 for correlation, 0 for no correlation).\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for cp in col_pairs:\n",
        "        c1 = cp[0]\n",
        "        c2= cp[1]\n",
        "        s1 = set(c1.split())\n",
        "        s2 = set(c2.split())\n",
        "        ns1 = len(s1)\n",
        "        ns2 = len(s2)\n",
        "        ni = len(set.intersection(s1, s2))\n",
        "        # calculate Jaccard coefficient\n",
        "        jac = ni / (ns1 + ns2 - ni)\n",
        "        # predict correlation if similar\n",
        "        if jac > 0.5:\n",
        "            predictions.append(1)\n",
        "        else:\n",
        "            predictions.append(0)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# log all metrics into summary for data subset\n",
        "def log_metrics(\n",
        "        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "        test_ratio, sub_test, test_name, lb, ub, pred_method,\n",
        "        out_path, training_time):\n",
        "    \"\"\" Predicts using baseline or model, writes metrics to file.\n",
        "\n",
        "    Args:\n",
        "        coeff: predict correlation according to this coefficient.\n",
        "        min_v1: lower bound on coefficient value for correlation.\n",
        "        max_v2: upper bound on p-value to be considered correlated.\n",
        "        mod_type: base type of language model used for prediction.\n",
        "        mod_name: precise name of language model used for prediction.\n",
        "        scenario: how training and test data relate to each other.\n",
        "        test_ratio: ratio of column pairs used for testing (not training).\n",
        "        sub_test: data frame with test cases, possibly a subset.\n",
        "        test_name: write this test name into result file.\n",
        "        lb: lower bound on a test-specific metric constraining test cases.\n",
        "        ub: upper bound on test-specific metric, constraining test cases.\n",
        "        pred_method: whether to use language model or simple baseline.\n",
        "        out_path: path to result output file (results are appended).\n",
        "    \"\"\"\n",
        "    sub_test.columns = [\n",
        "        'text_a', 'text_b', 'type1', 'type2', 'labels', 'length', 'nrtokens']\n",
        "    # print out a sample for later analysis\n",
        "    print(f'Sample for test {test_name}:')\n",
        "    sample = sub_test.sample(frac=0.1)\n",
        "    print(sample)\n",
        "    # predict correlation via baseline or model\n",
        "    sub_test = sub_test[['text_a', 'text_b', 'labels']]\n",
        "    samples = []\n",
        "    for _, r in sub_test.iterrows():\n",
        "        samples.append([r['text_a'], r['text_b']])\n",
        "    s_time = time.time()\n",
        "    if pred_method == 0:\n",
        "        preds = baseline(samples)\n",
        "    else:\n",
        "        preds = model.predict(samples)[0]\n",
        "    # log various performance metrics\n",
        "    t_time = time.time() - s_time\n",
        "    nr_samples = len(sub_test.index)\n",
        "    t_per_s = float(t_time) / nr_samples\n",
        "    f1 = metrics.f1_score(sub_test['labels'], preds)\n",
        "    pre = metrics.precision_score(sub_test['labels'], preds)\n",
        "    rec = metrics.recall_score(sub_test['labels'], preds)\n",
        "    acc = metrics.accuracy_score(sub_test['labels'], preds)\n",
        "    mcc = metrics.matthews_corrcoef(sub_test['labels'], preds)\n",
        "    # also log to local file\n",
        "    with open(out_path, 'a+') as file:\n",
        "        file.write(f'{coeff},{min_v1},{max_v2},\"{mod_type}\",' \\\n",
        "                f'\"{mod_name}\",\"{scenario}\",{test_ratio},' \\\n",
        "                f'\"{test_name}\",{pred_method},{lb},{ub},' \\\n",
        "                f'{f1},{pre},{rec},{acc},{mcc},{t_per_s},' \\\n",
        "                f'{training_time}\\n')\n",
        "\n",
        "\n",
        "def names_length(row):\n",
        "    \"\"\" Calculate combined length of column names.\n",
        "\n",
        "    Args:\n",
        "        row: contains information on one column pair.\n",
        "\n",
        "    Returns:\n",
        "        combined length of column names (in characters).\n",
        "    \"\"\"\n",
        "    return len(row['text_a']) + len(row['text_b'])\n",
        "\n",
        "def names_tokens(row):\n",
        "    \"\"\" Calculates number of tokens (separated by spaces).\n",
        "\n",
        "    Attention: this is not the number of tokens as calculated\n",
        "    by the tokenizer of the language model but an approximation.\n",
        "\n",
        "    Args:\n",
        "        row: contains information on one column pair.\n",
        "\n",
        "    Returns:\n",
        "        number of space-separated substrings in both column names.\n",
        "    \"\"\"\n",
        "    return row['text_a'].count(' ') + row['text_b'].count(' ')\n",
        "\n",
        "\n",
        "def run_experiment(src_path, coeff, min_v1, max_v2, mod_type, mod_name, scenario, test_ratio, use_types, out_path):\n",
        "    # print parameters\n",
        "    print(f'Coefficients: {coeff}')\n",
        "    print(f'Minimal value 1: {min_v1}')\n",
        "    print(f'Maximal value 2: {max_v2}')\n",
        "    print(f'Model type: {mod_type}')\n",
        "    print(f'Model name: {mod_name}')\n",
        "    print(f'Scenario: {scenario}')\n",
        "    print(f'Test ratio: {test_ratio}')\n",
        "\n",
        "    # initialize for deterministic results\n",
        "    seed = 42\n",
        "    rand.seed(seed)\n",
        "\n",
        "    # load data\n",
        "    data = pd.read_csv(src_path, sep=',')\n",
        "    data = data.sample(frac=1, random_state=seed)\n",
        "    data.columns = [\n",
        "        'dataid', 'datapath', 'nrrows', 'nrvals1', 'nrvals2',\n",
        "        'type1', 'type2', 'column1', 'column2', 'method',\n",
        "        'coefficient', 'pvalue', 'time']\n",
        "\n",
        "    # enrich column names if activated\n",
        "    if use_types:\n",
        "        data = data.apply(add_type, axis=1)\n",
        "\n",
        "    # Change the tokenizer and model here\n",
        "    tokenizer = AutoTokenizer.from_pretrained(mod_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(mod_name)\n",
        "\n",
        "    # filter data\n",
        "    data = data[data['method'] == coeff]\n",
        "    nr_total = len(data.index)\n",
        "    print(f'Nr. samples: {nr_total}')\n",
        "    print('Sample from filtered data:')\n",
        "    print(data.head())\n",
        "\n",
        "    # label data\n",
        "    def coefficient_label(row):\n",
        "        \"\"\" Label column pair as correlated or uncorrelated.\n",
        "\n",
        "        Args:\n",
        "            row: describes correlation between column pair.\n",
        "\n",
        "        Returns:\n",
        "            1 if correlated, 0 if not correlated.\n",
        "        \"\"\"\n",
        "        if abs(row['coefficient']) >= min_v1 and abs(row['pvalue']) <= max_v2:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    data['label'] = data.apply(coefficient_label, axis=1)\n",
        "\n",
        "    # split into test and training\n",
        "    if scenario == 'defsep':\n",
        "        train, test = def_split(data, test_ratio, seed)\n",
        "    elif scenario == 'datasep':\n",
        "        train, test = ds_split(data, test_ratio)\n",
        "    else:\n",
        "        raise ValueError(f'Undefined scenario: {scenario}')\n",
        "\n",
        "    train.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels']\n",
        "    test.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels']\n",
        "    print(train.head())\n",
        "    print(test.head())\n",
        "\n",
        "    # prepare loss scaling\n",
        "    lab_counts = train['labels'].value_counts()\n",
        "    nr_zeros = lab_counts.loc[0]\n",
        "    nr_ones = lab_counts.loc[1]\n",
        "    nr_all = float(len(train.index))\n",
        "    weights = [nr_all/nr_zeros, nr_all/nr_ones]\n",
        "\n",
        "    # train classification model\n",
        "    s_time = time.time()\n",
        "    model_args = ClassificationArgs(\n",
        "        num_train_epochs=5, train_batch_size=100, eval_batch_size=100,\n",
        "        overwrite_output_dir=True, manual_seed=seed,\n",
        "        evaluate_during_training=True, no_save=True)\n",
        "    classification_model = ClassificationModel(\n",
        "        mod_type, mod_name, weight=weights,\n",
        "        use_cuda=True, args=model_args)\n",
        "    classification_model.train_model(\n",
        "        train_df=train, eval_df=test, acc=metrics.accuracy_score,\n",
        "        rec=metrics.recall_score, pre=metrics.precision_score,\n",
        "        f1=metrics.f1_score)\n",
        "    training_time = time.time() - s_time\n",
        "\n",
        "    test['length'] = test.apply(names_length, axis=1)\n",
        "    test['nrtokens'] = test.apply(names_tokens, axis=1)\n",
        "\n",
        "    # Initialize result file\n",
        "    with open(out_path, 'w') as file:\n",
        "        file.write(\n",
        "            'coefficient,min_v1,max_v2,mod_type,mod_name,scenario,test_ratio,'\n",
        "            'test_name,pred_method,lb,ub,f1,precision,recall,accuracy,mcc,'\n",
        "            'prediction_time,training_time\\n')\n",
        "\n",
        "    # use simple baseline and model for prediction\n",
        "    for m in [0, 1]:\n",
        "        # use entire test set (redundant - for verification)\n",
        "        test_name = f'{m}-final'\n",
        "        log_metrics(\n",
        "            coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "            test_ratio, test, test_name, 0, 'inf', m, out_path, training_time)\n",
        "\n",
        "        # test for data types\n",
        "        for type1 in ['object', 'float64', 'int64', 'bool']:\n",
        "            for type2 in ['object', 'float64', 'int64', 'bool']:\n",
        "                sub_test = test.query(f'type1==\"{type1}\" and type2==\"{type2}\"')\n",
        "                if sub_test.shape[0]:\n",
        "                    test_name = f'Types{type1}-{type2}'\n",
        "                    log_metrics(\n",
        "                        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                        test_ratio, sub_test, test_name, -1, -1, m,\n",
        "                        out_path, training_time)\n",
        "\n",
        "        # test for different subsets\n",
        "        for q in [(0, 0.25), (0.25, 0.5), (0.5, 1)]:\n",
        "            qlb = q[0]\n",
        "            qub = q[1]\n",
        "            # column name length\n",
        "            lb = test['length'].quantile(qlb)\n",
        "            ub = test['length'].quantile(qub)\n",
        "            sub_test = test[(test['length'] >= lb) & (test['length'] <= ub)]\n",
        "            test_name = f'L{m}-{qlb}-{qub}'\n",
        "            log_metrics(\n",
        "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                test_ratio, sub_test, test_name, lb, ub, m, out_path, training_time)\n",
        "            # number of tokens in column names\n",
        "            lb = test['nrtokens'].quantile(qlb)\n",
        "            ub = test['nrtokens'].quantile(qub)\n",
        "            sub_test = test[(test['nrtokens'] >= lb) & (test['nrtokens'] <= ub)]\n",
        "            test_name = f'N{m}-{qlb}-{qub}'\n",
        "            log_metrics(\n",
        "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                test_ratio, sub_test, test_name, lb, ub, m, out_path, training_time)\n",
        "\n",
        "# Example usage in a Jupyter Notebook or Google Colab\n",
        "args = {\n",
        "    \"src_path\": \"/content/drive/My Drive/Colab Notebooks/Liter/correlations/correlationdata.csv\",\n",
        "    \"coeff\": \"pearson\",\n",
        "    \"min_v1\": 0.9,\n",
        "    \"max_v2\": 0.05,\n",
        "    \"mod_type\": \"Qwen\",\n",
        "    \"mod_name\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
        "    \"scenario\": \"defsep\",\n",
        "    \"test_ratio\": 0.2,\n",
        "    \"use_types\": 1,\n",
        "    \"out_path\": \"/content/drive/My Drive/Colab Notebooks/Liter/correlations/models\"\n",
        "}\n",
        "\n",
        "run_experiment(**args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "TPf0CrmHE7d6",
        "outputId": "ec27760e-8e0b-483f-d6f6-eb509ebfeb89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST' from 'transformers.models.camembert.modeling_camembert' (/usr/local/lib/python3.10/dist-packages/transformers/models/camembert/modeling_camembert.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a94892054b37>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationArgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from simpletransformers.classification.multi_label_classification_model import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mMultiLabelClassificationModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m from simpletransformers.classification.multi_modal_classification_model import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/simpletransformers/classification/multi_label_classification_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLabelClassificationArgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msweep_config_to_sweep_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m from simpletransformers.custom_models.models import (\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mAlbertForMultiLabelSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mBertForMultiLabelSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/simpletransformers/custom_models/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamembert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_camembert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCamembertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m from transformers.models.camembert.modeling_camembert import (\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mCAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m )\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST' from 'transformers.models.camembert.modeling_camembert' (/usr/local/lib/python3.10/dist-packages/transformers/models/camembert/modeling_camembert.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Aug 12, 2023\n",
        "\n",
        "@author: immanueltrummer\n",
        "'''\n",
        "from multiprocessing import set_start_method\n",
        "try:\n",
        "    set_start_method('spawn')\n",
        "except RuntimeError:\n",
        "    pass\n",
        "\n",
        "import argparse\n",
        "import sklearn.metrics as metrics\n",
        "import pandas as pd\n",
        "import random as rand\n",
        "import time\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "\n",
        "def add_type(row):\n",
        "    \"\"\" Enrich column name by adding column type.\n",
        "\n",
        "    Args:\n",
        "        row: describes correlation between two columns.\n",
        "\n",
        "    Returns:\n",
        "        row with enriched column names.\n",
        "    \"\"\"\n",
        "    row['column1'] = row['column1'] + ' ' + row['type1']\n",
        "    row['column2'] = row['column2'] + ' ' + row['type2']\n",
        "    return row\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\" Computes and returns a dictionary of metrics (accuracy, f1, precision, recall).\n",
        "\n",
        "    Args:\n",
        "        pred: predictions from the model.\n",
        "\n",
        "    Returns:\n",
        "        dictionary containing the calculated metrics.\n",
        "    \"\"\"\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = metrics.precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = metrics.accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "def def_split(data, test_ratio, seed):\n",
        "    \"\"\" Split data into training and test set.\n",
        "\n",
        "    With this approach, different column pairs from the\n",
        "    same data set may appear in training and test set.\n",
        "\n",
        "    Args:\n",
        "        data: a pandas dataframe containing all data.\n",
        "        test_ratio: ratio of test cases after split.\n",
        "        seed: random seed for deterministic results.\n",
        "\n",
        "    Returns:\n",
        "        a tuple containing training, then test data.\n",
        "    \"\"\"\n",
        "    print('Data sets in training and test may overlap')\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "      data[['column1', 'column2', 'type1', 'type2']], data['label'],\n",
        "      test_size=test_ratio, random_state=seed)\n",
        "    train = pd.concat([x_train, y_train], axis=1)\n",
        "    test = pd.concat([x_test, y_test], axis=1)\n",
        "    print(f'train shape: {train.shape}')\n",
        "    print(f'test shape: {test.shape}')\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def ds_split(data, test_ratio):\n",
        "    \"\"\" Split column pairs into training and test samples.\n",
        "\n",
        "    With this method, training and test set contain columns\n",
        "    of disjunct data sets, making prediction a bit harder.\n",
        "\n",
        "    Args:\n",
        "        data: a pandas dataframe containing all data.\n",
        "        test_ratio: ratio of test cases after splitting.\n",
        "\n",
        "    Returns:\n",
        "        a tuple containing training, then test set.\n",
        "    \"\"\"\n",
        "    print('Separating training and test sets by data')\n",
        "    counts = data['dataid'].value_counts()\n",
        "    print(f'Counts: {counts}')\n",
        "    print(f'Count.index: {counts.index}')\n",
        "    print(f'Count.index.values: {counts.index.values}')\n",
        "    print(f'counts.shape: {counts.shape}')\n",
        "    print(f'counts.iloc[0]: {counts.iloc[0]}')\n",
        "    nr_vals = len(counts)\n",
        "    nr_test_ds = int(nr_vals * test_ratio)\n",
        "    print(f'Nr. test data sets: {nr_test_ds}')\n",
        "    ds_ids = counts.index.values.tolist()\n",
        "    print(type(ds_ids))\n",
        "    print(ds_ids)\n",
        "    test_ds = rand.sample(ds_ids, nr_test_ds)\n",
        "    print(f'TestDS: {test_ds}')\n",
        "\n",
        "    def is_test(row):\n",
        "        if row['dataid'] in test_ds:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    data['istest'] = data.apply(is_test, axis=1)\n",
        "    train = data[data['istest'] == False]\n",
        "    test = data[data['istest'] == True]\n",
        "    print(f'train.shape: {train.shape}')\n",
        "    print(f'test.shape: {test.shape}')\n",
        "    print(train)\n",
        "    print(test)\n",
        "    return train[\n",
        "        ['column1', 'column2', 'type1', 'type2', 'label']], test[\n",
        "            ['column1', 'column2', 'type1', 'type2', 'label']]\n",
        "\n",
        "\n",
        "def baseline(col_pairs):\n",
        "    \"\"\" A simple baseline predicting correlation via Jaccard similarity.\n",
        "\n",
        "    Args:\n",
        "        col_pairs: list of tuples with column names.\n",
        "\n",
        "    Returns:\n",
        "        list of predictions (1 for correlation, 0 for no correlation).\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for cp in col_pairs:\n",
        "        c1 = cp[0]\n",
        "        c2= cp[1]\n",
        "        s1 = set(c1.split())\n",
        "        s2 = set(c2.split())\n",
        "        ns1 = len(s1)\n",
        "        ns2 = len(s2)\n",
        "        ni = len(set.intersection(s1, s2))\n",
        "        # calculate Jaccard coefficient\n",
        "        jac = ni / (ns1 + ns2 - ni)\n",
        "        # predict correlation if similar\n",
        "        if jac > 0.5:\n",
        "            predictions.append(1)\n",
        "        else:\n",
        "            predictions.append(0)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# log all metrics into summary for data subset\n",
        "def log_metrics(\n",
        "        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "        test_ratio, sub_test, test_name, lb, ub, pred_method,\n",
        "        out_path, training_time):\n",
        "    \"\"\" Predicts using baseline or model, writes metrics to file.\n",
        "\n",
        "    Args:\n",
        "        coeff: predict correlation according to this coefficient.\n",
        "        min_v1: lower bound on coefficient value for correlation.\n",
        "        max_v2: upper bound on p-value to be considered correlated.\n",
        "        mod_type: base type of language model used for prediction.\n",
        "        mod_name: precise name of language model used for prediction.\n",
        "        scenario: how training and test data relate to each other.\n",
        "        test_ratio: ratio of column pairs used for testing (not training).\n",
        "        sub_test: data frame with test cases, possibly a subset.\n",
        "        test_name: write this test name into result file.\n",
        "        lb: lower bound on a test-specific metric constraining test cases.\n",
        "        ub: upper bound on test-specific metric, constraining test cases.\n",
        "        pred_method: whether to use language model or simple baseline.\n",
        "        out_path: path to result output file (results are appended).\n",
        "        training_time: time taken to train the model.\n",
        "    \"\"\"\n",
        "    sub_test.columns = [\n",
        "        'text_a', 'text_b', 'type1', 'type2', 'labels', 'length', 'nrtokens']\n",
        "    # print out a sample for later analysis\n",
        "    print(f'Sample for test {test_name}:')\n",
        "    sample = sub_test.sample(frac=0.1)\n",
        "    print(sample)\n",
        "    # predict correlation via baseline or model\n",
        "    sub_test = sub_test[['text_a', 'text_b', 'labels']]\n",
        "    samples = []\n",
        "    for _, r in sub_test.iterrows():\n",
        "        samples.append([r['text_a'], r['text_b']])\n",
        "    s_time = time.time()\n",
        "    if pred_method == 0:\n",
        "        preds = baseline(samples)\n",
        "    else:\n",
        "        preds = model.predict(samples)[0]\n",
        "    # log various performance metrics\n",
        "    t_time = time.time() - s_time\n",
        "    nr_samples = len(sub_test.index)\n",
        "    t_per_s = float(t_time) / nr_samples\n",
        "    f1 = metrics.f1_score(sub_test['labels'], preds)\n",
        "    pre = metrics.precision_score(sub_test['labels'], preds)\n",
        "    rec = metrics.recall_score(sub_test['labels'], preds)\n",
        "    acc = metrics.accuracy_score(sub_test['labels'], preds)\n",
        "    mcc = metrics.matthews_corrcoef(sub_test['labels'], preds)\n",
        "    # also log to local file\n",
        "    with open(out_path, 'a+') as file:\n",
        "        file.write(f'{coeff},{min_v1},{max_v2},\"{mod_type}\",' \\\n",
        "                f'\"{mod_name}\",\"{scenario}\",{test_ratio},' \\\n",
        "                f'\"{test_name}\",{pred_method},{lb},{ub},' \\\n",
        "                f'{f1},{pre},{rec},{acc},{mcc},{t_per_s},' \\\n",
        "                f'{training_time}\\n')\n",
        "\n",
        "\n",
        "def names_length(row):\n",
        "    \"\"\" Calculate combined length of column names.\n",
        "\n",
        "    Args:\n",
        "        row: contains information on one column pair.\n",
        "\n",
        "    Returns:\n",
        "        combined length of column names (in characters).\n",
        "    \"\"\"\n",
        "    return len(row['text_a']) + len(row['text_b'])\n",
        "\n",
        "def names_tokens(row):\n",
        "    \"\"\" Calculates number of tokens (separated by spaces).\n",
        "\n",
        "    Attention: this is not the number of tokens as calculated\n",
        "    by the tokenizer of the language model but an approximation.\n",
        "\n",
        "    Args:\n",
        "        row: contains information on one column pair.\n",
        "\n",
        "    Returns:\n",
        "        number of space-separated substrings in both column names.\n",
        "    \"\"\"\n",
        "    return row['text_a'].count(' ') + row['text_b'].count(' ')\n",
        "\n",
        "\n",
        "def run_experiment(src_path, coeff, min_v1, max_v2, mod_type, mod_name, scenario, test_ratio, use_types, out_path):\n",
        "    # print parameters\n",
        "    print(f'Coefficients: {coeff}')\n",
        "    print(f'Minimal value 1: {min_v1}')\n",
        "    print(f'Maximal value 2: {max_v2}')\n",
        "    print(f'Model type: {mod_type}')\n",
        "    print(f'Model name: {mod_name}')\n",
        "    print(f'Scenario: {scenario}')\n",
        "    print(f'Test ratio: {test_ratio}')\n",
        "\n",
        "    # initialize for deterministic results\n",
        "    seed = 42\n",
        "    rand.seed(seed)\n",
        "\n",
        "    # load data\n",
        "    data = pd.read_csv(src_path, sep=',')\n",
        "    data = data.sample(frac=1, random_state=seed)\n",
        "    data.columns = [\n",
        "        'dataid', 'datapath', 'nrrows', 'nrvals1', 'nrvals2',\n",
        "        'type1', 'type2', 'column1', 'column2', 'method',\n",
        "        'coefficient', 'pvalue', 'time']\n",
        "\n",
        "    # enrich column names if activated\n",
        "    if use_types:\n",
        "        data = data.apply(add_type, axis=1)\n",
        "\n",
        "    # Change the tokenizer and model here\n",
        "    tokenizer = AutoTokenizer.from_pretrained(mod_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(mod_name, num_labels=2)\n",
        "\n",
        "    # filter data\n",
        "    data = data[data['method'] == coeff]\n",
        "    nr_total = len(data.index)\n",
        "    print(f'Nr. samples: {nr_total}')\n",
        "    print('Sample from filtered data:')\n",
        "    print(data.head())\n",
        "\n",
        "    # label data\n",
        "    def coefficient_label(row):\n",
        "        \"\"\" Label column pair as correlated or uncorrelated.\n",
        "\n",
        "        Args:\n",
        "            row: describes correlation between column pair.\n",
        "\n",
        "        Returns:\n",
        "            1 if correlated, 0 if not correlated.\n",
        "        \"\"\"\n",
        "        if abs(row['coefficient']) >= min_v1 and abs(row['pvalue']) <= max_v2:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    data['label'] = data.apply(coefficient_label, axis=1)\n",
        "\n",
        "    # split into test and training\n",
        "    if scenario == 'defsep':\n",
        "        train, test = def_split(data, test_ratio, seed)\n",
        "    elif scenario == 'datasep':\n",
        "        train, test = ds_split(data, test_ratio)\n",
        "    else:\n",
        "        raise ValueError(f'Undefined scenario: {scenario}')\n",
        "\n",
        "    train.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels']\n",
        "    test.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels']\n",
        "    print(train.head())\n",
        "    print(test.head())\n",
        "\n",
        "    # prepare dataset for transformers\n",
        "    train_encodings = tokenizer(train['text_a'].tolist(), train['text_b'].tolist(), truncation=True, padding=True)\n",
        "    test_encodings = tokenizer(test['text_a'].tolist(), test['text_b'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "    class Dataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    train_dataset = Dataset(train_encodings, train['labels'].tolist())\n",
        "    test_dataset = Dataset(test_encodings, test['labels'].tolist())\n",
        "\n",
        "    # prepare training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # train the model\n",
        "    s_time = time.time()\n",
        "    trainer.train()\n",
        "    training_time = time.time() - s_time\n",
        "\n",
        "    test['length'] = test.apply(names_length, axis=1)\n",
        "    test['nrtokens'] = test.apply(names_tokens, axis=1)\n",
        "\n",
        "    # Initialize result file\n",
        "    with open(out_path, 'w') as file:\n",
        "        file.write(\n",
        "            'coefficient,min_v1,max_v2,mod_type,mod_name,scenario,test_ratio,'\n",
        "            'test_name,pred_method,lb,ub,f1,precision,recall,accuracy,mcc,'\n",
        "            'prediction_time,training_time\\n')\n",
        "\n",
        "    # use simple baseline and model for prediction\n",
        "    for m in [0, 1]:\n",
        "        # use entire test set (redundant - for verification)\n",
        "        test_name = f'{m}-final'\n",
        "        log_metrics(\n",
        "            coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "            test_ratio, test, test_name, 0, 'inf', m, out_path, training_time)\n",
        "\n",
        "        # test for data types\n",
        "        for type1 in ['object', 'float64', 'int64', 'bool']:\n",
        "            for type2 in ['object', 'float64', 'int64', 'bool']:\n",
        "                sub_test = test.query(f'type1==\"{type1}\" and type2==\"{type2}\"')\n",
        "                if sub_test.shape[0]:\n",
        "                    test_name = f'Types{type1}-{type2}'\n",
        "                    log_metrics(\n",
        "                        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                        test_ratio, sub_test, test_name, -1, -1, m,\n",
        "                        out_path, training_time)\n",
        "\n",
        "        # test for different subsets\n",
        "        for q in [(0, 0.25), (0.25, 0.5), (0.5, 1)]:\n",
        "            qlb = q[0]\n",
        "            qub = q[1]\n",
        "            # column name length\n",
        "            lb = test['length'].quantile(qlb)\n",
        "            ub = test['length'].quantile(qub)\n",
        "            sub_test = test[(test['length'] >= lb) & (test['length'] <= ub)]\n",
        "            test_name = f'L{m}-{qlb}-{qub}'\n",
        "            log_metrics(\n",
        "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                test_ratio, sub_test, test_name, lb, ub, m, out_path, training_time)\n",
        "            # number of tokens in column names\n",
        "            lb = test['nrtokens'].quantile(qlb)\n",
        "            ub = test['nrtokens'].quantile(qub)\n",
        "            sub_test = test[(test['nrtokens'] >= lb) & (test['nrtokens'] <= ub)]\n",
        "            test_name = f'N{m}-{qlb}-{qub}'\n",
        "            log_metrics(\n",
        "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                test_ratio, sub_test, test_name, lb, ub, m, out_path, training_time)\n",
        "\n",
        "# Example usage in a Jupyter Notebook or Google Colab\n",
        "args = {\n",
        "    \"src_path\": \"/content/drive/My Drive/Colab Notebooks/Liter/correlations/correlationdata.csv\",\n",
        "    \"coeff\": \"pearson\",\n",
        "    \"min_v1\": 0.9,\n",
        "    \"max_v2\": 0.05,\n",
        "    \"mod_type\": \"Qwen\",\n",
        "    \"mod_name\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
        "    \"scenario\": \"defsep\",\n",
        "    \"test_ratio\": 0.2,\n",
        "    \"use_types\": 1,\n",
        "    \"out_path\": \"/content/drive/My Drive/Colab Notebooks/Liter/correlations/models\"\n",
        "}\n",
        "\n",
        "run_experiment(**args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KZCUokFKwdt",
        "outputId": "66107f31-854f-47a2-c99d-eeb12bfaec77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Coefficients: pearson\n",
            "Minimal value 1: 0.9\n",
            "Maximal value 2: 0.05\n",
            "Model type: Qwen\n",
            "Model name: Qwen/Qwen2-1.5B-Instruct\n",
            "Scenario: defsep\n",
            "Test ratio: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z9nWDIdMdPQ",
        "outputId": "f872dbd1-8982-4962-d1aa-58ecbdf2c25c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==1.1.1\n",
            "aiohappyeyeballs==2.4.3\n",
            "aiohttp==3.11.2\n",
            "aiosignal==1.3.1\n",
            "alabaster==1.0.0\n",
            "albucore==0.0.19\n",
            "albumentations==1.4.20\n",
            "altair==4.2.2\n",
            "annotated-types==0.7.0\n",
            "anyio==3.7.1\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.5.1\n",
            "arviz==0.20.0\n",
            "astropy==6.1.6\n",
            "astropy-iers-data==0.2024.11.18.0.35.2\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.3\n",
            "atpublic==4.1.0\n",
            "attrs==24.2.0\n",
            "audioread==3.0.1\n",
            "autograd==1.7.0\n",
            "babel==2.16.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.12.3\n",
            "bigframes==1.27.0\n",
            "bigquery-magics==0.4.0\n",
            "bleach==6.2.0\n",
            "blinker==1.9.0\n",
            "blis==0.7.11\n",
            "blosc2==2.7.1\n",
            "bokeh==3.6.1\n",
            "Bottleneck==1.4.2\n",
            "bqplot==0.12.43\n",
            "branca==0.8.0\n",
            "CacheControl==0.14.1\n",
            "cachetools==5.5.0\n",
            "catalogue==2.0.10\n",
            "certifi==2024.8.30\n",
            "cffi==1.17.1\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.4.0\n",
            "chex==0.1.87\n",
            "clarabel==0.9.0\n",
            "click==8.1.7\n",
            "cloudpathlib==0.20.0\n",
            "cloudpickle==3.1.0\n",
            "cmake==3.30.5\n",
            "cmdstanpy==1.2.4\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.6\n",
            "contourpy==1.3.1\n",
            "cryptography==43.0.3\n",
            "cuda-python==12.2.1\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda12x==12.2.0\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.5.4\n",
            "cycler==0.12.1\n",
            "cymem==2.0.8\n",
            "Cython==3.0.11\n",
            "dask==2024.10.0\n",
            "datascience==0.17.6\n",
            "datasets==3.1.0\n",
            "db-dtypes==1.3.1\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.8.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "Deprecated==1.2.15\n",
            "diffusers==0.31.0\n",
            "dill==0.3.8\n",
            "distro==1.9.0\n",
            "dlib==19.24.2\n",
            "dm-tree==0.1.8\n",
            "docker-pycreds==0.4.0\n",
            "docstring_parser==0.16\n",
            "docutils==0.21.2\n",
            "dopamine_rl==4.0.9\n",
            "duckdb==1.1.3\n",
            "earthengine-api==1.2.0\n",
            "easydict==1.13\n",
            "ecos==2.0.14\n",
            "editdistance==0.8.1\n",
            "eerepr==0.0.4\n",
            "einops==0.8.0\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889\n",
            "entrypoints==0.4\n",
            "et_xmlfile==2.0.0\n",
            "etils==1.10.0\n",
            "etuples==0.3.9\n",
            "eval_type_backport==0.2.0\n",
            "exceptiongroup==1.2.2\n",
            "fastai==2.7.18\n",
            "fastcore==1.7.20\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.20.0\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.2\n",
            "filelock==3.16.1\n",
            "firebase-admin==6.5.0\n",
            "Flask==3.0.3\n",
            "flatbuffers==24.3.25\n",
            "flax==0.8.5\n",
            "folium==0.18.0\n",
            "fonttools==4.55.0\n",
            "frozendict==2.4.6\n",
            "frozenlist==1.5.0\n",
            "fsspec==2024.9.0\n",
            "future==1.0.0\n",
            "gast==0.6.0\n",
            "gcsfs==2024.10.0\n",
            "GDAL==3.6.4\n",
            "gdown==5.2.0\n",
            "geemap==0.35.1\n",
            "gensim==4.3.3\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==1.0.1\n",
            "geopy==2.4.1\n",
            "gin-config==0.5.0\n",
            "gitdb==4.0.11\n",
            "GitPython==3.1.43\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.10\n",
            "google-api-core==2.19.2\n",
            "google-api-python-client==2.151.0\n",
            "google-auth==2.27.0\n",
            "google-auth-httplib2==0.2.0\n",
            "google-auth-oauthlib==1.2.1\n",
            "google-cloud-aiplatform==1.71.1\n",
            "google-cloud-bigquery==3.25.0\n",
            "google-cloud-bigquery-connection==1.16.1\n",
            "google-cloud-bigquery-storage==2.27.0\n",
            "google-cloud-bigtable==2.27.0\n",
            "google-cloud-core==2.4.1\n",
            "google-cloud-datastore==2.20.1\n",
            "google-cloud-firestore==2.19.0\n",
            "google-cloud-functions==1.18.1\n",
            "google-cloud-iam==2.16.1\n",
            "google-cloud-language==2.15.1\n",
            "google-cloud-pubsub==2.27.1\n",
            "google-cloud-resource-manager==1.13.1\n",
            "google-cloud-storage==2.8.0\n",
            "google-cloud-translate==3.17.0\n",
            "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
            "google-crc32c==1.6.0\n",
            "google-generativeai==0.8.3\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.2\n",
            "googleapis-common-protos==1.66.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.3\n",
            "greenlet==3.1.1\n",
            "grpc-google-iam-v1==0.13.1\n",
            "grpcio==1.68.0\n",
            "grpcio-status==1.62.3\n",
            "gspread==6.0.2\n",
            "gspread-dataframe==3.3.1\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h11==0.14.0\n",
            "h5netcdf==1.4.1\n",
            "h5py==3.12.1\n",
            "holidays==0.61\n",
            "holoviews==1.20.0\n",
            "html5lib==1.1\n",
            "httpcore==1.0.7\n",
            "httpimport==1.4.0\n",
            "httplib2==0.22.0\n",
            "httpx==0.27.2\n",
            "huggingface-hub==0.26.2\n",
            "humanize==4.11.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==9.2.0\n",
            "idna==3.10\n",
            "imageio==2.36.0\n",
            "imageio-ffmpeg==0.5.1\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.12.4\n",
            "imgaug==0.4.0\n",
            "immutabledict==4.2.1\n",
            "importlib_metadata==8.5.0\n",
            "importlib_resources==6.4.5\n",
            "imutils==0.5.4\n",
            "inflect==7.4.0\n",
            "iniconfig==2.0.0\n",
            "intel-cmplr-lib-ur==2025.0.0\n",
            "intel-openmp==2025.0.0\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==5.5.6\n",
            "ipyleaflet==0.19.2\n",
            "ipyparallel==8.8.0\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jax==0.4.33\n",
            "jax-cuda12-pjrt==0.4.33\n",
            "jax-cuda12-plugin==0.4.33\n",
            "jaxlib==0.4.33\n",
            "jeepney==0.7.1\n",
            "jellyfish==1.1.0\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.4\n",
            "jiter==0.7.1\n",
            "joblib==1.4.2\n",
            "jsonpatch==1.33\n",
            "jsonpickle==4.0.0\n",
            "jsonpointer==3.0.0\n",
            "jsonschema==4.23.0\n",
            "jsonschema-specifications==2024.10.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-leaflet==0.19.2\n",
            "jupyter-server==1.24.0\n",
            "jupyter_core==5.7.2\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.13\n",
            "kaggle==1.6.17\n",
            "kagglehub==0.3.4\n",
            "keras==3.5.0\n",
            "keyring==23.5.0\n",
            "kiwisolver==1.4.7\n",
            "langchain==0.3.7\n",
            "langchain-core==0.3.19\n",
            "langchain-text-splitters==0.3.2\n",
            "langcodes==3.4.1\n",
            "langsmith==0.1.143\n",
            "language_data==1.2.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-24.10.1-py3-none-manylinux_2_28_x86_64.whl\n",
            "librosa==0.10.2.post1\n",
            "lightgbm==4.5.0\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.43.0\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==5.3.0\n",
            "marisa-trie==1.2.1\n",
            "Markdown==3.7\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==3.0.2\n",
            "matplotlib==3.8.0\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==1.1.1\n",
            "mdit-py-plugins==0.4.2\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==3.0.2\n",
            "mizani==0.13.0\n",
            "mkl==2025.0.0\n",
            "ml-dtypes==0.4.1\n",
            "mlxtend==0.23.3\n",
            "more-itertools==10.5.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.1.0\n",
            "multidict==6.1.0\n",
            "multipledispatch==1.0.0\n",
            "multiprocess==0.70.16\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.10\n",
            "music21==9.3.0\n",
            "namex==0.0.8\n",
            "natsort==8.4.0\n",
            "nbclassic==1.1.0\n",
            "nbclient==0.10.0\n",
            "nbconvert==7.16.4\n",
            "nbformat==5.10.4\n",
            "ndindex==1.9.2\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.4.2\n",
            "nibabel==5.3.2\n",
            "nltk==3.9.1\n",
            "notebook==6.5.5\n",
            "notebook_shim==0.2.4\n",
            "numba==0.60.0\n",
            "numexpr==2.10.1\n",
            "numpy==1.26.4\n",
            "nvidia-cublas-cu12==12.6.3.3\n",
            "nvidia-cuda-cupti-cu12==12.6.80\n",
            "nvidia-cuda-nvcc-cu12==12.6.77\n",
            "nvidia-cuda-runtime-cu12==12.6.77\n",
            "nvidia-cudnn-cu12==9.5.1.17\n",
            "nvidia-cufft-cu12==11.3.0.4\n",
            "nvidia-curand-cu12==10.3.7.77\n",
            "nvidia-cusolver-cu12==11.7.1.2\n",
            "nvidia-cusparse-cu12==12.5.4.2\n",
            "nvidia-nccl-cu12==2.23.4\n",
            "nvidia-nvjitlink-cu12==12.6.77\n",
            "nvtx==0.2.10\n",
            "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.10.0-py3-none-any.whl\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "openai==1.54.4\n",
            "opencv-contrib-python==4.10.0.84\n",
            "opencv-python==4.10.0.84\n",
            "opencv-python-headless==4.10.0.84\n",
            "openpyxl==3.1.5\n",
            "opentelemetry-api==1.28.2\n",
            "opentelemetry-sdk==1.28.2\n",
            "opentelemetry-semantic-conventions==0.49b2\n",
            "opt_einsum==3.4.0\n",
            "optax==0.2.4\n",
            "optree==0.13.1\n",
            "orbax-checkpoint==0.6.4\n",
            "orjson==3.10.11\n",
            "osqp==0.6.7.post3\n",
            "packaging==24.2\n",
            "pandas==2.2.2\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.24.0\n",
            "pandas-stubs==2.2.2.240909\n",
            "pandocfilters==1.5.1\n",
            "panel==1.5.4\n",
            "param==2.1.1\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==1.0.1\n",
            "peewee==3.17.8\n",
            "peft==0.13.2\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "pillow==11.0.0\n",
            "platformdirs==4.3.6\n",
            "plotly==5.24.1\n",
            "plotnine==0.14.1\n",
            "pluggy==1.5.0\n",
            "polars==1.9.0\n",
            "pooch==1.8.2\n",
            "portpicker==1.5.2\n",
            "preshed==3.0.9\n",
            "prettytable==3.12.0\n",
            "proglog==0.1.10\n",
            "progressbar2==4.5.0\n",
            "prometheus_client==0.21.0\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.48\n",
            "propcache==0.2.0\n",
            "prophet==1.1.6\n",
            "proto-plus==1.25.0\n",
            "protobuf==4.25.5\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.10\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==17.0.0\n",
            "pyarrow-hotfix==0.6\n",
            "pyasn1==0.6.1\n",
            "pyasn1_modules==0.4.1\n",
            "pycocotools==2.0.8\n",
            "pycparser==2.22\n",
            "pydantic==2.9.2\n",
            "pydantic_core==2.23.4\n",
            "pydata-google-auth==1.8.2\n",
            "pydeck==0.9.1\n",
            "pydot==3.0.2\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.21.1\n",
            "pyerfa==2.0.1.5\n",
            "pygame==2.6.1\n",
            "pygit2==1.16.0\n",
            "Pygments==2.18.0\n",
            "PyGObject==3.42.1\n",
            "PyJWT==2.10.0\n",
            "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "pylibcugraph-cu12==24.10.0\n",
            "pylibraft-cu12==24.10.0\n",
            "pymc==5.18.2\n",
            "pymystem3==0.2.0\n",
            "pynvjitlink-cu12==0.4.0\n",
            "pyogrio==0.10.0\n",
            "PyOpenGL==3.1.7\n",
            "pyOpenSSL==24.2.1\n",
            "pyparsing==3.2.0\n",
            "pyperclip==1.9.0\n",
            "pyproj==3.7.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pyspark==3.5.3\n",
            "pytensor==2.26.3\n",
            "pytest==8.3.3\n",
            "python-apt==0.0.0\n",
            "python-box==7.2.0\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.4\n",
            "python-utils==3.9.0\n",
            "pytz==2024.2\n",
            "pyviz_comms==3.0.3\n",
            "PyYAML==6.0.2\n",
            "pyzmq==24.0.1\n",
            "qdldl==0.1.7.post4\n",
            "ratelim==0.1.6\n",
            "referencing==0.35.1\n",
            "regex==2024.9.11\n",
            "requests==2.32.3\n",
            "requests-oauthlib==1.3.1\n",
            "requests-toolbelt==1.0.0\n",
            "requirements-parser==0.9.0\n",
            "rich==13.9.4\n",
            "rmm-cu12==24.10.0\n",
            "rpds-py==0.21.0\n",
            "rpy2==3.4.2\n",
            "rsa==4.9\n",
            "safetensors==0.4.5\n",
            "scikit-image==0.24.0\n",
            "scikit-learn==1.5.2\n",
            "scipy==1.13.1\n",
            "scooby==0.10.0\n",
            "scs==3.2.7\n",
            "seaborn==0.13.2\n",
            "SecretStorage==3.3.1\n",
            "Send2Trash==1.8.3\n",
            "sentence-transformers==3.2.1\n",
            "sentencepiece==0.2.0\n",
            "sentry-sdk==2.18.0\n",
            "seqeval==1.2.2\n",
            "setproctitle==1.3.4\n",
            "shap==0.46.0\n",
            "shapely==2.0.6\n",
            "shellingham==1.5.4\n",
            "simple-parsing==0.1.6\n",
            "simpletransformers==0.70.1\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "slicer==0.0.8\n",
            "smart-open==7.0.5\n",
            "smmap==5.0.1\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==2.2.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.6\n",
            "soxr==0.5.0.post1\n",
            "spacy==3.7.5\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "Sphinx==8.1.3\n",
            "sphinxcontrib-applehelp==2.0.0\n",
            "sphinxcontrib-devhelp==2.0.0\n",
            "sphinxcontrib-htmlhelp==2.1.0\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==2.0.0\n",
            "sphinxcontrib-serializinghtml==2.0.0\n",
            "SQLAlchemy==2.0.36\n",
            "sqlglot==25.1.0\n",
            "sqlparse==0.5.2\n",
            "srsly==2.4.8\n",
            "stanio==0.5.1\n",
            "statsmodels==0.14.4\n",
            "streamlit==1.40.2\n",
            "StrEnum==0.4.15\n",
            "stringzilla==3.10.10\n",
            "sympy==1.13.1\n",
            "tables==3.10.1\n",
            "tabulate==0.9.0\n",
            "tbb==2022.0.0\n",
            "tcmlib==1.2.0\n",
            "tenacity==9.0.0\n",
            "tensorboard==2.17.1\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorboardX==2.6.2.2\n",
            "tensorflow==2.17.1\n",
            "tensorflow-datasets==4.9.7\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.1\n",
            "tensorflow-metadata==1.13.1\n",
            "tensorflow-probability==0.24.0\n",
            "tensorstore==0.1.68\n",
            "termcolor==2.5.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.17.0\n",
            "thinc==8.2.5\n",
            "threadpoolctl==3.5.0\n",
            "tifffile==2024.9.20\n",
            "timm==1.0.11\n",
            "tinycss2==1.4.0\n",
            "tokenizers==0.20.3\n",
            "toml==0.10.2\n",
            "tomli==2.1.0\n",
            "toolz==0.12.1\n",
            "torch @ https://download.pytorch.org/whl/cu121_full/torch-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl\n",
            "torchaudio @ https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchvision @ https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl\n",
            "tornado==6.3.3\n",
            "tqdm==4.66.6\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.46.2\n",
            "tweepy==4.14.0\n",
            "typeguard==4.4.1\n",
            "typer==0.13.0\n",
            "types-pytz==2024.2.0.20241003\n",
            "types-setuptools==75.5.0.20241122\n",
            "typing_extensions==4.12.2\n",
            "tzdata==2024.2\n",
            "tzlocal==5.2\n",
            "uc-micro-py==1.0.3\n",
            "umf==0.9.0\n",
            "uritemplate==4.1.1\n",
            "urllib3==2.2.3\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wandb==0.18.7\n",
            "wasabi==1.1.3\n",
            "watchdog==6.0.0\n",
            "wcwidth==0.2.13\n",
            "weasel==0.4.1\n",
            "webcolors==24.11.1\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "Werkzeug==3.1.3\n",
            "widgetsnbextension==3.6.10\n",
            "wordcloud==1.9.4\n",
            "wrapt==1.16.0\n",
            "xarray==2024.10.0\n",
            "xarray-einstats==0.8.0\n",
            "xgboost==2.1.2\n",
            "xlrd==2.0.1\n",
            "xxhash==3.5.0\n",
            "xyzservices==2024.9.0\n",
            "yarl==1.17.2\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.49\n",
            "zipp==3.21.0\n"
          ]
        }
      ]
    }
  ]
}