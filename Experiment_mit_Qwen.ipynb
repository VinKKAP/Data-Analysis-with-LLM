{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOC7xw9XDRLaM2+Iio/Z7fF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinKKAP/Data-Analysis-with-LLM/blob/main/Experiment_mit_Qwen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC-ECJCiC5QZ",
        "outputId": "ce4c6671-dd2e-4698-83f2-096c3e5efd95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")  # Assuming device 0\n",
        "else:\n",
        "    print(\"CUDA is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMEslf08Phwn",
        "outputId": "3fda0479-7011-43b8-aedd-5b705166ce37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!\n",
            "Number of CUDA devices: 1\n",
            "Current CUDA device: 0\n",
            "Device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Data-Analysis-with-LLM"
      ],
      "metadata": {
        "id": "5z72Ft90EY-e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VinKKAP/Data-Analysis-with-LLM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9jpnfJuDCI3",
        "outputId": "b1ecdc0b-f77c-4936-bd87-d2884adca85e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Data-Analysis-with-LLM'...\n",
            "remote: Enumerating objects: 230, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 230 (delta 3), reused 10 (delta 2), pack-reused 215 (from 1)\u001b[K\n",
            "Receiving objects: 100% (230/230), 57.45 MiB | 25.48 MiB/s, done.\n",
            "Resolving deltas: 100% (72/72), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/Data-Analysis-with-LLM/Paper/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvAX46fhDG-7",
        "outputId": "c370ff6e-6412-4836-98f1-a51bbbd33553"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.13.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2.13.1)\n",
            "Requirement already satisfied: pandas==2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 3)) (3.20.3)\n",
            "Requirement already satisfied: scikit_learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: scipy==1.11.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 5)) (1.11.1)\n",
            "Requirement already satisfied: simpletransformers==0.64.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.64.3)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (2.0.1)\n",
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 8)) (4.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (0.70.14)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.2->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.2->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.2->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.2->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.2.2->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2024.9.11)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2.17.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.13.3)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.18.7)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (from simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.40.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 8)) (0.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (0.45.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (18.1.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.2->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 1)) (2024.8.30)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (4.3.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (3.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (5.5.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (11.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (6.0.0)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (6.3.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (1.68.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (4.0.11)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers==0.64.3->-r /content/Data-Analysis-with-LLM/Paper/requirements.txt (line 6)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Data-Analysis-with-LLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9LEqB1GEjdj",
        "outputId": "897d2f09-3e43-4cab-d9b7-6ee8964549cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Data-Analysis-with-LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python colab_kernel_launcher.py <coeff> <min_v1> <max_v2> <mod_type> <mod_name> <scenario> <test_ratio> <use_types> <out_path>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMKd2D0eFWI1",
        "outputId": "29422f7f-bdf2-4165-b40c-a42d9f88a643"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `<'\n",
            "/bin/bash: -c: line 1: `python colab_kernel_launcher.py <coeff> <min_v1> <max_v2> <mod_type> <mod_name> <scenario> <test_ratio> <use_types> <out_path>'\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install qwen-tokenizer Qwen2-1.5B-Instruct"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8NFu3pzFj4m",
        "outputId": "6b70f86c-bf0b-4de8-b06f-bf4a22867efb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qwen-tokenizer\n",
            "  Using cached qwen_tokenizer-0.2.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement Qwen2-1.5B-Instruct (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for Qwen2-1.5B-Instruct\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Aug 12, 2023\n",
        "\n",
        "@author: immanueltrummer\n",
        "'''\n",
        "from multiprocessing import set_start_method\n",
        "try:\n",
        "    set_start_method('spawn')\n",
        "except RuntimeError:\n",
        "    pass\n",
        "\n",
        "import argparse\n",
        "import sklearn.metrics as metrics\n",
        "import pandas as pd\n",
        "import random as rand\n",
        "import time\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from simpletransformers.classification import ClassificationArgs, ClassificationModel\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "\n",
        "\n",
        "def add_type(row):\n",
        "    \"\"\" Enrich column name by adding column type.\n",
        "\n",
        "    Args:\n",
        "        row: describes correlation between two columns.\n",
        "\n",
        "    Returns:\n",
        "        row with enriched column names.\n",
        "    \"\"\"\n",
        "    row['column1'] = row['column1'] + ' ' + row['type1']\n",
        "    row['column2'] = row['column2'] + ' ' + row['type2']\n",
        "    return row\n",
        "\n",
        "\n",
        "def def_split(data, test_ratio, seed):\n",
        "    \"\"\" Split data into training and test set.\n",
        "\n",
        "    With this approach, different column pairs from the\n",
        "    same data set may appear in training and test set.\n",
        "\n",
        "    Args:\n",
        "        data: a pandas dataframe containing all data.\n",
        "        test_ratio: ratio of test cases after split.\n",
        "        seed: random seed for deterministic results.\n",
        "\n",
        "    Returns:\n",
        "        a tuple containing training, then test data.\n",
        "    \"\"\"\n",
        "    print('Data sets in training and test may overlap')\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "      data[['column1', 'column2', 'type1', 'type2']], data['label'],\n",
        "      test_size=test_ratio, random_state=seed)\n",
        "    train = pd.concat([x_train, y_train], axis=1)\n",
        "    test = pd.concat([x_test, y_test], axis=1)\n",
        "    print(f'train shape: {train.shape}')\n",
        "    print(f'test shape: {test.shape}')\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def ds_split(data, test_ratio):\n",
        "    \"\"\" Split column pairs into training and test samples.\n",
        "\n",
        "    With this method, training and test set contain columns\n",
        "    of disjunct data sets, making prediction a bit harder.\n",
        "\n",
        "    Args:\n",
        "        data: a pandas dataframe containing all data.\n",
        "        test_ratio: ratio of test cases after splitting.\n",
        "\n",
        "    Returns:\n",
        "        a tuple containing training, then test set.\n",
        "    \"\"\"\n",
        "    print('Separating training and test sets by data')\n",
        "    counts = data['dataid'].value_counts()\n",
        "    print(f'Counts: {counts}')\n",
        "    print(f'Count.index: {counts.index}')\n",
        "    print(f'Count.index.values: {counts.index.values}')\n",
        "    print(f'counts.shape: {counts.shape}')\n",
        "    print(f'counts.iloc[0]: {counts.iloc[0]}')\n",
        "    nr_vals = len(counts)\n",
        "    nr_test_ds = int(nr_vals * test_ratio)\n",
        "    print(f'Nr. test data sets: {nr_test_ds}')\n",
        "    ds_ids = counts.index.values.tolist()\n",
        "    print(type(ds_ids))\n",
        "    print(ds_ids)\n",
        "    test_ds = rand.sample(ds_ids, nr_test_ds)\n",
        "    print(f'TestDS: {test_ds}')\n",
        "\n",
        "    def is_test(row):\n",
        "        if row['dataid'] in test_ds:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    data['istest'] = data.apply(is_test, axis=1)\n",
        "    train = data[data['istest'] == False]\n",
        "    test = data[data['istest'] == True]\n",
        "    print(f'train.shape: {train.shape}')\n",
        "    print(f'test.shape: {test.shape}')\n",
        "    print(train)\n",
        "    print(test)\n",
        "    return train[\n",
        "        ['column1', 'column2', 'type1', 'type2', 'label']], test[\n",
        "            ['column1', 'column2', 'type1', 'type2', 'label']]\n",
        "\n",
        "\n",
        "def baseline(col_pairs):\n",
        "    \"\"\" A simple baseline predicting correlation via Jaccard similarity.\n",
        "\n",
        "    Args:\n",
        "        col_pairs: list of tuples with column names.\n",
        "\n",
        "    Returns:\n",
        "        list of predictions (1 for correlation, 0 for no correlation).\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for cp in col_pairs:\n",
        "        c1 = cp[0]\n",
        "        c2= cp[1]\n",
        "        s1 = set(c1.split())\n",
        "        s2 = set(c2.split())\n",
        "        ns1 = len(s1)\n",
        "        ns2 = len(s2)\n",
        "        ni = len(set.intersection(s1, s2))\n",
        "        # calculate Jaccard coefficient\n",
        "        jac = ni / (ns1 + ns2 - ni)\n",
        "        # predict correlation if similar\n",
        "        if jac > 0.5:\n",
        "            predictions.append(1)\n",
        "        else:\n",
        "            predictions.append(0)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# log all metrics into summary for data subset\n",
        "def log_metrics(\n",
        "        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "        test_ratio, sub_test, test_name, lb, ub, pred_method,\n",
        "        out_path, training_time):\n",
        "    \"\"\" Predicts using baseline or model, writes metrics to file.\n",
        "\n",
        "    Args:\n",
        "        coeff: predict correlation according to this coefficient.\n",
        "        min_v1: lower bound on coefficient value for correlation.\n",
        "        max_v2: upper bound on p-value to be considered correlated.\n",
        "        mod_type: base type of language model used for prediction.\n",
        "        mod_name: precise name of language model used for prediction.\n",
        "        scenario: how training and test data relate to each other.\n",
        "        test_ratio: ratio of column pairs used for testing (not training).\n",
        "        sub_test: data frame with test cases, possibly a subset.\n",
        "        test_name: write this test name into result file.\n",
        "        lb: lower bound on a test-specific metric constraining test cases.\n",
        "        ub: upper bound on test-specific metric, constraining test cases.\n",
        "        pred_method: whether to use language model or simple baseline.\n",
        "        out_path: path to result output file (results are appended).\n",
        "    \"\"\"\n",
        "    sub_test.columns = [\n",
        "        'text_a', 'text_b', 'type1', 'type2', 'labels', 'length', 'nrtokens']\n",
        "    # print out a sample for later analysis\n",
        "    print(f'Sample for test {test_name}:')\n",
        "    sample = sub_test.sample(frac=0.1)\n",
        "    print(sample)\n",
        "    # predict correlation via baseline or model\n",
        "    sub_test = sub_test[['text_a', 'text_b', 'labels']]\n",
        "    samples = []\n",
        "    for _, r in sub_test.iterrows():\n",
        "        samples.append([r['text_a'], r['text_b']])\n",
        "    s_time = time.time()\n",
        "    if pred_method == 0:\n",
        "        preds = baseline(samples)\n",
        "    else:\n",
        "        preds = model.predict(samples)[0]\n",
        "    # log various performance metrics\n",
        "    t_time = time.time() - s_time\n",
        "    nr_samples = len(sub_test.index)\n",
        "    t_per_s = float(t_time) / nr_samples\n",
        "    f1 = metrics.f1_score(sub_test['labels'], preds)\n",
        "    pre = metrics.precision_score(sub_test['labels'], preds)\n",
        "    rec = metrics.recall_score(sub_test['labels'], preds)\n",
        "    acc = metrics.accuracy_score(sub_test['labels'], preds)\n",
        "    mcc = metrics.matthews_corrcoef(sub_test['labels'], preds)\n",
        "    # also log to local file\n",
        "    with open(out_path, 'a+') as file:\n",
        "        file.write(f'{coeff},{min_v1},{max_v2},\"{mod_type}\",' \\\n",
        "                f'\"{mod_name}\",\"{scenario}\",{test_ratio},' \\\n",
        "                f'\"{test_name}\",{pred_method},{lb},{ub},' \\\n",
        "                f'{f1},{pre},{rec},{acc},{mcc},{t_per_s},' \\\n",
        "                f'{training_time}\\n')\n",
        "\n",
        "\n",
        "def names_length(row):\n",
        "    \"\"\" Calculate combined length of column names.\n",
        "\n",
        "    Args:\n",
        "        row: contains information on one column pair.\n",
        "\n",
        "    Returns:\n",
        "        combined length of column names (in characters).\n",
        "    \"\"\"\n",
        "    return len(row['text_a']) + len(row['text_b'])\n",
        "\n",
        "def names_tokens(row):\n",
        "    \"\"\" Calculates number of tokens (separated by spaces).\n",
        "\n",
        "    Attention: this is not the number of tokens as calculated\n",
        "    by the tokenizer of the language model but an approximation.\n",
        "\n",
        "    Args:\n",
        "        row: contains information on one column pair.\n",
        "\n",
        "    Returns:\n",
        "        number of space-separated substrings in both column names.\n",
        "    \"\"\"\n",
        "    return row['text_a'].count(' ') + row['text_b'].count(' ')\n",
        "\n",
        "\n",
        "def run_experiment(src_path, coeff, min_v1, max_v2, mod_type, mod_name, scenario, test_ratio, use_types, out_path):\n",
        "    # print parameters\n",
        "    print(f'Coefficients: {coeff}')\n",
        "    print(f'Minimal value 1: {min_v1}')\n",
        "    print(f'Maximal value 2: {max_v2}')\n",
        "    print(f'Model type: {mod_type}')\n",
        "    print(f'Model name: {mod_name}')\n",
        "    print(f'Scenario: {scenario}')\n",
        "    print(f'Test ratio: {test_ratio}')\n",
        "\n",
        "    # initialize for deterministic results\n",
        "    seed = 42\n",
        "    rand.seed(seed)\n",
        "\n",
        "    # load data\n",
        "    data = pd.read_csv(src_path, sep=',')\n",
        "    data = data.sample(frac=1, random_state=seed)\n",
        "    data.columns = [\n",
        "        'dataid', 'datapath', 'nrrows', 'nrvals1', 'nrvals2',\n",
        "        'type1', 'type2', 'column1', 'column2', 'method',\n",
        "        'coefficient', 'pvalue', 'time']\n",
        "\n",
        "    # enrich column names if activated\n",
        "    if use_types:\n",
        "        data = data.apply(add_type, axis=1)\n",
        "\n",
        "    # Change the tokenizer and model here\n",
        "    tokenizer = AutoTokenizer.from_pretrained(mod_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(mod_name)\n",
        "\n",
        "    # filter data\n",
        "    data = data[data['method'] == coeff]\n",
        "    nr_total = len(data.index)\n",
        "    print(f'Nr. samples: {nr_total}')\n",
        "    print('Sample from filtered data:')\n",
        "    print(data.head())\n",
        "\n",
        "    # label data\n",
        "    def coefficient_label(row):\n",
        "        \"\"\" Label column pair as correlated or uncorrelated.\n",
        "\n",
        "        Args:\n",
        "            row: describes correlation between column pair.\n",
        "\n",
        "        Returns:\n",
        "            1 if correlated, 0 if not correlated.\n",
        "        \"\"\"\n",
        "        if abs(row['coefficient']) >= min_v1 and abs(row['pvalue']) <= max_v2:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    data['label'] = data.apply(coefficient_label, axis=1)\n",
        "\n",
        "    # split into test and training\n",
        "    if scenario == 'defsep':\n",
        "        train, test = def_split(data, test_ratio, seed)\n",
        "    elif scenario == 'datasep':\n",
        "        train, test = ds_split(data, test_ratio)\n",
        "    else:\n",
        "        raise ValueError(f'Undefined scenario: {scenario}')\n",
        "\n",
        "    train.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels']\n",
        "    test.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels']\n",
        "    print(train.head())\n",
        "    print(test.head())\n",
        "\n",
        "    # prepare loss scaling\n",
        "    lab_counts = train['labels'].value_counts()\n",
        "    nr_zeros = lab_counts.loc[0]\n",
        "    nr_ones = lab_counts.loc[1]\n",
        "    nr_all = float(len(train.index))\n",
        "    weights = [nr_all/nr_zeros, nr_all/nr_ones]\n",
        "\n",
        "    # train classification model\n",
        "    s_time = time.time()\n",
        "    model_args = ClassificationArgs(\n",
        "        num_train_epochs=5, train_batch_size=100, eval_batch_size=100,\n",
        "        overwrite_output_dir=True, manual_seed=seed,\n",
        "        evaluate_during_training=True, no_save=True)\n",
        "    classification_model = ClassificationModel(\n",
        "        mod_type, mod_name, weight=weights,\n",
        "        use_cuda=True, args=model_args)\n",
        "    classification_model.train_model(\n",
        "        train_df=train, eval_df=test, acc=metrics.accuracy_score,\n",
        "        rec=metrics.recall_score, pre=metrics.precision_score,\n",
        "        f1=metrics.f1_score)\n",
        "    training_time = time.time() - s_time\n",
        "\n",
        "    test['length'] = test.apply(names_length, axis=1)\n",
        "    test['nrtokens'] = test.apply(names_tokens, axis=1)\n",
        "\n",
        "    # Initialize result file\n",
        "    with open(out_path, 'w') as file:\n",
        "        file.write(\n",
        "            'coefficient,min_v1,max_v2,mod_type,mod_name,scenario,test_ratio,'\n",
        "            'test_name,pred_method,lb,ub,f1,precision,recall,accuracy,mcc,'\n",
        "            'prediction_time,training_time\\n')\n",
        "\n",
        "    # use simple baseline and model for prediction\n",
        "    for m in [0, 1]:\n",
        "        # use entire test set (redundant - for verification)\n",
        "        test_name = f'{m}-final'\n",
        "        log_metrics(\n",
        "            coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "            test_ratio, test, test_name, 0, 'inf', m, out_path, training_time)\n",
        "\n",
        "        # test for data types\n",
        "        for type1 in ['object', 'float64', 'int64', 'bool']:\n",
        "            for type2 in ['object', 'float64', 'int64', 'bool']:\n",
        "                sub_test = test.query(f'type1==\"{type1}\" and type2==\"{type2}\"')\n",
        "                if sub_test.shape[0]:\n",
        "                    test_name = f'Types{type1}-{type2}'\n",
        "                    log_metrics(\n",
        "                        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                        test_ratio, sub_test, test_name, -1, -1, m,\n",
        "                        out_path, training_time)\n",
        "\n",
        "        # test for different subsets\n",
        "        for q in [(0, 0.25), (0.25, 0.5), (0.5, 1)]:\n",
        "            qlb = q[0]\n",
        "            qub = q[1]\n",
        "            # column name length\n",
        "            lb = test['length'].quantile(qlb)\n",
        "            ub = test['length'].quantile(qub)\n",
        "            sub_test = test[(test['length'] >= lb) & (test['length'] <= ub)]\n",
        "            test_name = f'L{m}-{qlb}-{qub}'\n",
        "            log_metrics(\n",
        "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                test_ratio, sub_test, test_name, lb, ub, m, out_path, training_time)\n",
        "            # number of tokens in column names\n",
        "            lb = test['nrtokens'].quantile(qlb)\n",
        "            ub = test['nrtokens'].quantile(qub)\n",
        "            sub_test = test[(test['nrtokens'] >= lb) & (test['nrtokens'] <= ub)]\n",
        "            test_name = f'N{m}-{qlb}-{qub}'\n",
        "            log_metrics(\n",
        "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                test_ratio, sub_test, test_name, lb, ub, m, out_path, training_time)\n",
        "\n",
        "# Example usage in a Jupyter Notebook or Google Colab\n",
        "args = {\n",
        "    \"src_path\": \"/content/drive/My Drive/Colab Notebooks/Liter/correlations/correlationdata.csv\",\n",
        "    \"coeff\": \"pearson\",\n",
        "    \"min_v1\": 0.9,\n",
        "    \"max_v2\": 0.05,\n",
        "    \"mod_type\": \"Qwen\",\n",
        "    \"mod_name\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
        "    \"scenario\": \"defsep\",\n",
        "    \"test_ratio\": 0.2,\n",
        "    \"use_types\": 1,\n",
        "    \"out_path\": \"/content/drive/My Drive/Colab Notebooks/Liter/correlations/models\"\n",
        "}\n",
        "\n",
        "run_experiment(**args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "TPf0CrmHE7d6",
        "outputId": "5cc5c4b0-2801-48f3-d23d-7dbb9bf3bdf7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'simpletransformers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a94892054b37>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationArgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassificationModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simpletransformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}