{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOAJYK5kphLUd1uI9wDk/3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinKKAP/Data-Analysis-with-LLM/blob/main/Experiment_Run1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYE1cvtOwjqz",
        "outputId": "c1949bb4-7aa2-4054-fd87-12570254dcc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VinKKAP/Data-Analysis-with-LLM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XfzTN_Yw0Zm",
        "outputId": "0b86a0c0-5463-4fd5-c1a9-421423aa4f7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Data-Analysis-with-LLM'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 361 (delta 23), reused 34 (delta 16), pack-reused 311 (from 1)\u001b[K\n",
            "Receiving objects: 100% (361/361), 57.58 MiB | 23.53 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWznSoK3w4FQ",
        "outputId": "7e5ae3f4-7157-415b-a5fe-803f1d0fe6ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Created on Aug 12, 2023\n",
        "\n",
        "@author: immanueltrummer\n",
        "'''\n",
        "from multiprocessing import set_start_method\n",
        "try:\n",
        "    set_start_method('spawn')\n",
        "except RuntimeError:\n",
        "    pass\n",
        "\n",
        "import argparse\n",
        "import sklearn.metrics as metrics\n",
        "import pandas as pd\n",
        "import random as rand\n",
        "import time\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "from simpletransformers.classification import (\n",
        "    ClassificationModel, ClassificationArgs\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Laden der .env-Datei\n",
        "load_dotenv()\n",
        "\n",
        "# Abrufen der API-SchlÃ¼ssel aus der Umgebung\n",
        "huggingface_api_key = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(huggingface_api_key)\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key=wandb_api_key)\n",
        "\n",
        "# Initialize wandb run\n",
        "wandb.init(project=\"Distilbert_run1\")\n",
        "\n",
        "\n",
        "def add_type(row):\n",
        "    \"\"\" Enrich column name by adding column type.\n",
        "\n",
        "    Args:\n",
        "        row: describes correlation between two columns.\n",
        "\n",
        "    Returns:\n",
        "        row with enriched column names.\n",
        "    \"\"\"\n",
        "    row['column1'] = row['column1'] + ' ' + row['type1']\n",
        "    row['column2'] = row['column2'] + ' ' + row['type2']\n",
        "    return row\n",
        "\n",
        "\n",
        "def def_split(data, test_ratio, seed):\n",
        "    \"\"\" Split data into training and test set.\n",
        "\n",
        "    With this approach, different column pairs from the\n",
        "    same data set may appear in training and test set.\n",
        "\n",
        "    Args:\n",
        "        data: a pandas dataframe containing all data.\n",
        "        test_ratio: ratio of test cases after split.\n",
        "        seed: random seed for deterministic results.\n",
        "\n",
        "    Returns:\n",
        "        a tuple containing training, then test data.\n",
        "    \"\"\"\n",
        "    print('Data sets in training and test may overlap')\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "      data[['column1', 'column2', 'type1', 'type2']], data['label'],\n",
        "      test_size=test_ratio, random_state=seed)\n",
        "    train = pd.concat([x_train, y_train], axis=1)\n",
        "    test = pd.concat([x_test, y_test], axis=1)\n",
        "    print(f'train shape: {train.shape}')\n",
        "    print(f'test shape: {test.shape}')\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def ds_split(data, test_ratio):\n",
        "    \"\"\" Split column pairs into training and test samples.\n",
        "\n",
        "    With this method, training and test set contain columns\n",
        "    of disjunct data sets, making prediction a bit harder.\n",
        "\n",
        "    Args:\n",
        "        data: a pandas dataframe containing all data.\n",
        "        test_ratio: ratio of test cases after splitting.\n",
        "\n",
        "    Returns:\n",
        "        a tuple containing training, then test set.\n",
        "    \"\"\"\n",
        "    print('Separating training and test sets by data')\n",
        "    counts = data['dataid'].value_counts()\n",
        "    print(f'Counts: {counts}')\n",
        "    print(f'Count.index: {counts.index}')\n",
        "    print(f'Count.index.values: {counts.index.values}')\n",
        "    print(f'counts.shape: {counts.shape}')\n",
        "    print(f'counts.iloc[0]: {counts.iloc[0]}')\n",
        "    nr_vals = len(counts)\n",
        "    nr_test_ds = int(nr_vals * test_ratio)\n",
        "    print(f'Nr. test data sets: {nr_test_ds}')\n",
        "    ds_ids = counts.index.values.tolist()\n",
        "    print(type(ds_ids))\n",
        "    print(ds_ids)\n",
        "    test_ds = rand.sample(ds_ids, nr_test_ds)\n",
        "    print(f'TestDS: {test_ds}')\n",
        "\n",
        "    def is_test(row):\n",
        "        if row['dataid'] in test_ds:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    data['istest'] = data.apply(is_test, axis=1)\n",
        "    train = data[data['istest'] == False]\n",
        "    test = data[data['istest'] == True]\n",
        "    print(f'train.shape: {train.shape}')\n",
        "    print(f'test.shape: {test.shape}')\n",
        "    print(train)\n",
        "    print(test)\n",
        "    return train[\n",
        "        ['column1', 'column2', 'type1', 'type2', 'label']], test[\n",
        "            ['column1', 'column2', 'type1', 'type2', 'label']]\n",
        "\n",
        "\n",
        "def baseline(col_pairs):\n",
        "    \"\"\" A simple baseline predicting correlation via Jaccard similarity.\n",
        "\n",
        "    Args:\n",
        "        col_pairs: list of tuples with column names.\n",
        "\n",
        "    Returns:\n",
        "        list of predictions (1 for correlation, 0 for no correlation).\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for cp in col_pairs:\n",
        "        c1 = cp[0]\n",
        "        c2= cp[1]\n",
        "        s1 = set(c1.split())\n",
        "        s2 = set(c2.split())\n",
        "        ns1 = len(s1)\n",
        "        ns2 = len(s2)\n",
        "        ni = len(set.intersection(s1, s2))\n",
        "        # calculate Jaccard coefficient\n",
        "        jac = ni / (ns1 + ns2 - ni)\n",
        "        # predict correlation if similar\n",
        "        if jac > 0.5:\n",
        "            predictions.append(1)\n",
        "        else:\n",
        "            predictions.append(0)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# log all metrics into summary for data subset\n",
        "def log_metrics(\n",
        "        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "        test_ratio, sub_test, test_name, lb, ub, pred_method,\n",
        "        out_path):\n",
        "    \"\"\" Predicts using baseline or model, writes metrics to file.\n",
        "\n",
        "    Args:\n",
        "        coeff: predict correlation according to this coefficient.\n",
        "        min_v1: lower bound on coefficient value for correlation.\n",
        "        max_v2: upper bound on p-value to be considered correlated.\n",
        "        mod_type: base type of language model used for prediction.\n",
        "        mod_name: precise name of language model used for prediction.\n",
        "        scenario: how training and test data relate to each other.\n",
        "        test_ratio: ratio of column pairs used for testing (not training).\n",
        "        sub_test: data frame with test cases, possibly a subset.\n",
        "        test_name: write this test name into result file.\n",
        "        lb: lower bound on a test-specific metric constraining test cases.\n",
        "        ub: upper bound on test-specific metric, constraining test cases.\n",
        "        pred_metho: whether to use language model or simple baseline.\n",
        "        out_path: path to result output file (results are appended).\n",
        "    \"\"\"\n",
        "    sub_test.columns = [\n",
        "        'text_a', 'text_b', 'type1', 'type2', 'labels', 'length', 'nrtokens']\n",
        "    # print out a sample for later analysis\n",
        "    print(f'Sample for test {test_name}:')\n",
        "    sample = sub_test.sample(frac=0.1)\n",
        "    print(sample)\n",
        "    # predict correlation via baseline or model\n",
        "    sub_test = sub_test[['text_a', 'text_b', 'labels']]\n",
        "    samples = []\n",
        "    for _, r in sub_test.iterrows():\n",
        "        samples.append([r['text_a'], r['text_b']])\n",
        "    s_time = time.time()\n",
        "    if pred_method == 0:\n",
        "        preds = baseline(samples)\n",
        "    else:\n",
        "        preds = model.predict(samples)[0]\n",
        "    # log various performance metrics\n",
        "    t_time = time.time() - s_time\n",
        "    nr_samples = len(sub_test.index)\n",
        "    t_per_s = float(t_time) / nr_samples\n",
        "    f1 = metrics.f1_score(sub_test['labels'], preds)\n",
        "    pre = metrics.precision_score(sub_test['labels'], preds)\n",
        "    rec = metrics.recall_score(sub_test['labels'], preds)\n",
        "    acc = metrics.accuracy_score(sub_test['labels'], preds)\n",
        "    mcc = metrics.matthews_corrcoef(sub_test['labels'], preds)\n",
        "    # also log to local file\n",
        "    with open(out_path, 'a+') as file:\n",
        "        file.write(f'{coeff},{min_v1},{max_v2},\"{mod_type}\",' \\\n",
        "                f'\"{mod_name}\",\"{scenario}\",{test_ratio},' \\\n",
        "                f'\"{test_name}\",{pred_method},{lb},{ub},' \\\n",
        "                f'{f1},{pre},{rec},{acc},{mcc},{t_per_s},' \\\n",
        "                f'{training_time}\\n')\n",
        "\n",
        "\n",
        "def names_length(row):\n",
        "    \"\"\" Calculate combined length of column names.\n",
        "\n",
        "    Args:\n",
        "        row: contains information on one column pair.\n",
        "\n",
        "    Returns:\n",
        "        combined length of column names (in characters).\n",
        "    \"\"\"\n",
        "    return len(row['text_a']) + len(row['text_b'])\n",
        "\n",
        "def names_tokens(row):\n",
        "    \"\"\" Calculates number of tokens (separated by spaces).\n",
        "\n",
        "    Attention: this is not the number of tokens as calculated\n",
        "    by the tokenizer of the language model but an approximation.\n",
        "\n",
        "    Args:\n",
        "        row: contains information on one column pair.\n",
        "\n",
        "    Returns:\n",
        "        number of space-separated substrings in both column names.\n",
        "    \"\"\"\n",
        "    return row['text_a'].count(' ') + row['text_b'].count(' ')\n",
        "\n",
        "\n",
        "def chi_square_test(predictions, labels, sex):\n",
        "    \"\"\" Perform Chi-Square test to check if there is a significant difference between sexes in predictions.\n",
        "\n",
        "    Args:\n",
        "        predictions: list of model predictions.\n",
        "        labels: list of true labels.\n",
        "        sex: list of sex corresponding to each prediction.\n",
        "\n",
        "    Returns:\n",
        "        p-value of the Chi-Square test.\n",
        "    \"\"\"\n",
        "    contingency_table = pd.crosstab(pd.Series(sex), pd.Series(predictions))\n",
        "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
        "    return p\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('src_path', type=str, help='Path to source file')\n",
        "    parser.add_argument(\n",
        "        'coeff', type=str, help='Correlation coefficient (e.g., \"pearson\")')\n",
        "    parser.add_argument(\n",
        "        'min_v1', type=float,\n",
        "        help='Minimal coefficient value for correlation (e.g., 0.9)')\n",
        "    parser.add_argument(\n",
        "        'max_v2', type=float,\n",
        "        help='Maximal p-value for correlation (e.g., 0.05)')\n",
        "    parser.add_argument(\n",
        "        'mod_type', type=str,\n",
        "        help='Type of language model used for prediction (e.g., \"roberta\")')\n",
        "    parser.add_argument(\n",
        "        'mod_name', type=str,\n",
        "        help='Language model used for prediction (e.g., \"robert-base\")')\n",
        "    parser.add_argument(\n",
        "        'scenario', type=str,\n",
        "        help='Default separation (\"defsep\") or by data set (\"datasep\")')\n",
        "    parser.add_argument(\n",
        "        'test_ratio', type=float,\n",
        "        help='Ratio of samples used for testing (e.g., 0.2), not training')\n",
        "    parser.add_argument(\n",
        "        'use_types', type=int, help='Use types for prediction (1) or not (0)')\n",
        "    parser.add_argument(\n",
        "        'out_path', type=str, help='Path to output file containing results')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # get command line parameters\n",
        "    coeff = args.coeff\n",
        "    min_v1 = args.min_v1\n",
        "    max_v2 = args.max_v2\n",
        "    mod_type = args.mod_type\n",
        "    mod_name = args.mod_name\n",
        "    scenario = args.scenario\n",
        "    test_ratio = args.test_ratio\n",
        "    print(f'Coefficients: {coeff}')\n",
        "    print(f'Minimal value 1: {min_v1}')\n",
        "    print(f'Maximal value 2: {max_v2}')\n",
        "    print(f'Model type: {mod_type}')\n",
        "    print(f'Model name: {mod_name}')\n",
        "    print(f'Scenario: {scenario}')\n",
        "    print(f'Test ratio: {test_ratio}')\n",
        "\n",
        "    # initialize for deterministic results\n",
        "    seed = 42\n",
        "    rand.seed(seed)\n",
        "\n",
        "    # load data\n",
        "    data = pd.read_csv(args.src_path, sep = ',')\n",
        "    data = data.sample(frac=1, random_state=seed)\n",
        "    data.columns = [\n",
        "        'dataid', 'datapath', 'nrrows', 'nrvals1', 'nrvals2',\n",
        "        'type1', 'type2', 'column1', 'column2', 'method',\n",
        "        'coefficient', 'pvalue', 'time', 'sex']\n",
        "\n",
        "    # enrich column names if activated\n",
        "    if args.use_types:\n",
        "        data = data.apply(add_type, axis=1)\n",
        "\n",
        "    # filter data\n",
        "    data = data[data['method']==coeff]\n",
        "    nr_total = len(data.index)\n",
        "    print(f'Nr. samples: {nr_total}')\n",
        "    print('Sample from filtered data:')\n",
        "    print(data.head())\n",
        "\n",
        "    # label data\n",
        "    def coefficient_label(row):\n",
        "        \"\"\" Label column pair as correlated or uncorrelated.\n",
        "\n",
        "        Args:\n",
        "            row: describes correlation between column pair.\n",
        "\n",
        "        Returns:\n",
        "            1 if correlated, 0 if not correlated.\n",
        "        \"\"\"\n",
        "        if abs(row['coefficient']) >= min_v1 and abs(row['pvalue']) <= max_v2:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    data['label'] = data.apply(coefficient_label, axis=1)\n",
        "\n",
        "    # split into test and training\n",
        "    if scenario == 'defsep':\n",
        "        train, test = def_split(data, test_ratio, seed)\n",
        "    elif scenario == 'datasep':\n",
        "        train, test = ds_split(data, test_ratio)\n",
        "    else:\n",
        "        raise ValueError(f'Undefined scenario: {scenario}')\n",
        "\n",
        "    train.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels']\n",
        "    test.columns = ['text_a', 'text_b', 'type1', 'type2', 'labels', 'sex']\n",
        "    print(train.head())\n",
        "    print(test.head())\n",
        "\n",
        "    # prepare loss scaling\n",
        "    lab_counts = train['labels'].value_counts()\n",
        "    nr_zeros = lab_counts.loc[0]\n",
        "    nr_ones = lab_counts.loc[1]\n",
        "    nr_all = float(len(train.index))\n",
        "    weights = [nr_all/nr_zeros, nr_all/nr_ones]\n",
        "\n",
        "    # train classification model\n",
        "    s_time = time.time()\n",
        "    model_args = ClassificationArgs(\n",
        "        num_train_epochs=5, train_batch_size=100, eval_batch_size=100,\n",
        "        overwrite_output_dir=True, manual_seed=seed,\n",
        "        evaluate_during_training=True, no_save=True)\n",
        "    model = ClassificationModel(\n",
        "        mod_type, mod_name, weight=weights,\n",
        "        use_cuda = True, args=model_args)\n",
        "    model.train_model(\n",
        "        train_df=train, eval_df=test, acc=metrics.accuracy_score,\n",
        "        rec=metrics.recall_score, pre=metrics.precision_score,\n",
        "        f1=metrics.f1_score)\n",
        "    training_time = time.time() - s_time\n",
        "\n",
        "    test['length'] = test.apply(names_length, axis=1)\n",
        "    test['nrtokens'] = test.apply(names_tokens, axis=1)\n",
        "\n",
        "    # Initialize result file\n",
        "    with open(args.out_path, 'w') as file:\n",
        "        file.write(\n",
        "            'coefficient,min_v1,max_v2,mod_type,mod_name,scenario,test_ratio,'\n",
        "            'test_name,pred_method,lb,ub,f1,precision,recall,accuracy,mcc,'\n",
        "            'prediction_time,training_time\\n')\n",
        "\n",
        "    # use simple baseline and model for prediction\n",
        "    for m in [0, 1]:\n",
        "        # use entire test set (redundant - for verification)\n",
        "        test_name = f'{m}-final'\n",
        "        log_metrics(\n",
        "            coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "            test_ratio, test, test_name, 0, 'inf', m, args.out_path)\n",
        "\n",
        "        # test for data types\n",
        "        for type1 in ['object', 'float64', 'int64', 'bool']:\n",
        "            for type2 in ['object', 'float64', 'int64', 'bool']:\n",
        "                sub_test = test.query(f'type1==\"{type1}\" and type2==\"{type2}\"')\n",
        "                if sub_test.shape[0]:\n",
        "                    test_name = f'Types{type1}-{type2}'\n",
        "                    log_metrics(\n",
        "                        coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                        test_ratio, sub_test, test_name, -1, -1, m,\n",
        "                        args.out_path)\n",
        "\n",
        "        # test for different subsets\n",
        "        for q in [(0, 0.25), (0.25, 0.5), (0.5, 1)]:\n",
        "            qlb = q[0]\n",
        "            qub = q[1]\n",
        "            # column name length\n",
        "            lb = test['length'].quantile(qlb)\n",
        "            ub = test['length'].quantile(qub)\n",
        "            sub_test = test[(test['length'] >= lb) & (test['length'] <= ub)]\n",
        "            test_name = f'L{m}-{qlb}-{qub}'\n",
        "            log_metrics(\n",
        "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                test_ratio, sub_test, test_name, lb, ub, m, args.out_path)\n",
        "            # number of tokens in column names\n",
        "            lb = test['nrtokens'].quantile(qlb)\n",
        "            ub = test['nrtokens'].quantile(qub)\n",
        "            sub_test = test[(test['nrtokens'] >= lb) & (test['nrtokens'] <= ub)]\n",
        "            test_name = f'N{m}-{qlb}-{qub}'\n",
        "            log_metrics(\n",
        "                coeff, min_v1, max_v2, mod_type, mod_name, scenario,\n",
        "                test_ratio, sub_test, test_name, lb, ub, m, args.out_path)\n",
        "\n",
        "    # Collect predictions and perform Chi-Square test\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_sex = []\n",
        "    for sex in test['sex'].unique():\n",
        "        sub_test = test[test['sex'] == sex]\n",
        "        samples = [[r['text_a'], r['text_b']] for _, r in sub_test.iterrows()]\n",
        "        preds = model.predict(samples)[0]\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(sub_test['labels'].tolist())\n",
        "        all_sex.extend([sex] * len(preds))\n",
        "\n",
        "    p_value = chi_square_test(all_preds, all_labels, all_sex)\n",
        "    print(f'Chi-Square test p-value: {p_value}')\n",
        "\n",
        "    def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='binary')\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Example usage in a Jupyter Notebook or Google Colab\n",
        "args = {\n",
        "    \"src_path\": \"/content/Data-Analysis-with-LLM/experiment/data/corresult_output_file.csv\",\n",
        "    \"coeff\": \"pearson\",\n",
        "    \"min_v1\": 0.9,\n",
        "    \"max_v2\": 0.05,\n",
        "    \"mod_type\": \"distilbert\",\n",
        "    \"mod_name\": \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    \"scenario\": \"defsep\",\n",
        "    \"test_ratio\": 0.2,\n",
        "    \"use_types\": 1,\n",
        "    \"out_path\": \"/content/drive/My Drive/Colab Notebooks/Liter/correlations/models\"\n",
        "}\n",
        "\n",
        "run_experiment(**args)"
      ],
      "metadata": {
        "id": "k480Tdz0y0tl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}