---
bibliography: My EndNote Library.bib
fontsize: 11pt
linestretch: 1.5
format:
  pdf:
    documentclass: report
    keep-tex: true
    titlepage: false
    header-includes:
    - |
      \usepackage{titling}
      \pretitle{\begin{titlepage}\begin{center}\Large\scshape \textbf{Seminararbeit}\\*[2mm]
      \Large\scshape \textbf{M.Sc.-Studiengang "`Betriebswirtschaftslehre"'}\\*[5mm]
      \large Universität Hamburg\\*[0mm]
      \large Fakultät für Betriebswirtschaft\\*[0mm]
      \large Lehrstuhl für Statistik mit Anwendung in der Betriebswirtschaftslehre\\*[0mm]
      \large \emph{Prof. Dr. Martin Spindler}\\*[30mm]
      \bf \LARGE\scshape Data Profiling mit Large Language Models - Eine Studie am Beispiel des US Gender-Pay-Gap\\*[12mm]
      \large Gutachter: Prof. Dr. Martin Spindler\\*[0mm]\end{center}\vspace{2cm}
      \begin{tabbing}
      \textbf{eingereicht von:} \hspace{6.5cm}\= \textbf{betreut von:}\\*[2mm]
      Vincent-Konstantin Kapp \> Prof. Dr. Martin Spindler\\*[2mm]
      7778027 \\*[2mm]
      Methfesselstraße 16 \> \textbf{Abgabedatum, Ort:} \\*[2mm]
      20257 Hamburg \> 07.01.2025, Hamburg \\*[2mm]
      0152-56761933\\*[2mm]
      \\*[2mm]
      \end{tabbing}\end{titlepage}}
      \posttitle{}
    geometry: 
      - "a4paper"
      - left=35mm
      - right=35mm
      - top=4cm
      - bottom=4cm
    lof: true
    lot: true
    toc: true 
number-sections: true

execute:
    eval: true
    echo: true
    warning: false
jupyter: python3

abstract: |
  Datengetriebene Prozesse können Unternehmen nicht nur dabei helfen, ihre Prozesse zu optimieren, sondern auch bei der Entwicklung neuer Strategien. Um festzustellen, welche Daten in den Datenbanken vorliegen und wie diese zueinander stehen, braucht es Mitarbeiter*innen, die einzeln überprüfen, welche Daten vorhanden sind und in welchen Zusammenhängen sie basierend auf Common Sense existieren. Data Profiling stellt Unternehmen mit historisch gewachsenen Datenbanksystemen vor Herausforderungen für die datengestützte Transformation. Können die aktuellen Entwicklungen von Large Language Modellen (LLMs) dazu beitragen, automatisiert die Zusammenhänge von Spalten zu erkennen? Um diese Frage zu beantworten, widmet sich diese Seminararbeit, aufbauend auf der Arbeit von Trummer (2024), der Untersuchung, wie gut LLMs anhand einer Case Study die Korrelation von Daten erkennen können. Dazu wird ein LLM auf Basis von GPT-3 trainiert und mit einem Benchmark-Verfahren verglichen. Die Ergebnisse zeigen, dass LLMs eine hohe Genauigkeit bei der Korrelationserkennung aufweisen und damit Unternehmen bei der Datenanalyse unterstützen können.
---

# Einleitung
In Zeiten von Big Data ist es für Unternehmen von großer Bedeutung, ihre Datenbestände zu analysieren und zu verstehen. Datengetriebene Prozesse können Unternehmen nicht nur dabei helfen, ihre Prozesse zu optimieren, sondern auch bei der Entwicklung neuer Strategien. Ein signifikanter Teil der Daten in Unternehmen liegt in Datenbanken als strukturierte Daten vor z.B. in Form von Tabellen. Eine vielzahl von Tools ermöglicht es, diese Daten zu analysieren und zu visualisieren. Jedoch verwendet jedes Tool seine eigene query language. Beim Verwenden dieser Sprache ermöglicht es Nutzerinnen eine vielzahl von Datenanalseoperationen an strukturierten Daten zu durchzuführen. Diese query Sprache in natürlicher Sprache zu formulieren, bietet viel Potenzial für Unternehmen.
Dadurch können Nutzerinnen festellen, welche Daten in den Datenbanken vorliegen und wie diese zueinander stehen, braucht es Mitarbeiter*innen, die einzeln überprüfen, welche Daten vorhanden sind und in welchen Zusammenhängen sie basierend auf Common Sense existieren. Data Profiling stellt Unternehmen mit historisch gewachsenen Datenbanksystemen vor Herausforderungen. Die aktuellen Entwicklungen von Large Language Modellen (LLMs) könnten dazu beitragen, automatisiert die Zusammenhänge von Spalten zu erkennen. Dies könnte Unternehmen bei der datengestützten Transformation unterstützen. LLMs können eine universelle lösung darstellen, um eine vielzahhl von Datenanalysen zu unterstützen, in unterschiedlichen typen von strukturierten Datentypen. Das analysieren von Daten mit der erstellung von Anfragen von LLMs in natürlicher Sprache, das Modell kann die Fragen in SQL übersetzen, ausführen und die Ergebnisse präsentieren.

                            Frage
Data Structure -------Natürliche Sprache Query Interface

                          Query

Data   ----------------Data processing Tool


                          Ergebnisse

# Hintergrund
Ein signifikanter anteil der weltweit sturukturierten Daten befindet sich in Tabellen wieder. Diese Informationsquellen effektiv zu nutzen stellt Unternehmen, insbesondere Großunternehmen, vor der Herausforderung die benöteten fähigkeiten zu entwickeln. Hierfür ist es essentiell die tabellarischen Daten effizient und akkurat zu verstehen, zu verarbeiten, zu analysieren und einen wertschöpfenden Output zu generieren. Im Bezug zu den stettig schneller wachsenden Datenmengen und den historisch gewachsenen Informationssystemen, ist es eine komplexe aufgabe für die Anwendenden, um Textinhalte, numerische Werte, Formate oder Formeln in Tabellen zu verstehen.


Die generierung von Code und Text, bietet anwendern nun neue möglichkeiten ihre arbeitweise zu verändern, zu verbessern und zu automatisieren. 



Welche möglichkeiten die neuen durchbrüche ermöglichen wird gerade erst schrittweise erkannt. LLMs bieten im bezug zu tabelarischen Daten die möglichkeit anwendern dabei zu helfen Datenbanken besser und schneller zu verstehen.  
## Data Profiling

## Language Models
[@RN5576].

# Nutzung von NLP für Datenbankanalyse
Die durchbrüche von Large Language Models (LLMs) biete neue möglichkeiten in der verarbeitung von natürliche Sprache (NLP) und maschinellen Lernen (ML), dies eröffnet bemerkenswerte Erfolge beim verstehen und generieren von Text, Code etc.. 
LLMs arbeiten mit linearen Textsequenzen, um Tabellen für die Modelle nutzbar zu machen, müssen sie aus dem strukturierten, zweidimensionalen Format linearisiert werden z.B. CSV oder JSON. Sehr große Tabellen bringen die Modelle wegen der hohen Anzahl der Tokens an ihre grenzen. 

// Unterstützen  können hier Strategien wie Trunkierung, Retrieval-Augmentation und die segmentierung großer Tabellen.// 

Das Trainieren von Modellen auf tabellenspezifischen Daten und Multi-Task-Finetuning verbessern  die Anpassungsfähigkeit der Modelle. Hierfür werden die Modelle mit realen Datensätzen und einer omptimierten Trainingsstragie trainiert, um Tabllendaten besser zu verstehen.

// Training von LLMs für tabellarische Daten (Table-GPT und TableLlama)// 


LLMs können zum einen dabei helfen Tabellendaten zu interpretieren und zu verarbetien in dem sie die Datentypen in den Spalten Identifizieren, die "Entität" der Spalten miteinander verküpfen oder die Tabellendaten strukturieren. 
Zum anderen können LLMs dabei helfen den Output aus den Daten zu analysieren oder die Schlussfolgerung zu überprüfen in dem sie eine logische und semantische Verbindungen in Tabellen zu erkennen. 

// TabFact und WikiTableQuestions bewerten die Leistungsfähigkeit der Modelle //
TaBERT 

## Semantik Parsing 





## Erkennen von Korrelationen in Daten
[@RN5578].



## LLM-gesteuerte Tabellen-Agenten 
Agentenarchitekturen: LLMs können als Agenten agieren, die komplexe tabellarische Aufgaben in kleinere Schritte zerlegen und externe Werkzeuge nutzen.
Iterative Ansätze: Systeme wie ReAct analysieren und optimieren ihre Entscheidungen iterativ, bis sie das gewünschte Ergebnis erreichen.
Integration von Aktionen: LLM-Agenten können benutzerdefinierte Aktionen oder APIs nutzen, um Tabellen effizienter zu manipulieren.

## Heterogenität in Datensätzen: am Beispiel des US Gender-Pay-Gap

Modelle benötigen bei besonders heterogenen Datensätzen einen größeren Kontext um Sinnvolle zusammenhänge in den Datensätzen zu identifizieren. Dies liegt daran das es zum einen eine vielzahl von Datentypen gibt und zum anderen das die zusammenhänge nicht trivial sind. Am Beipiel des Genderpaygaps ist neben der heterogenität der Datenformate, die komplexität des Themas die wessentliche herausforderung bei der analyse. Die korrelationserkkung von nicht trivialen beziehung, die ungleiche verteilung der Daten (Bias),  die größe der Datenmengen, aber auch die schwierigkeit die abgeleiteten Korrelationen Nachvollziehbar und Erklärbar für die dritte zu gestallten. 

BACH PAPER



# Methodik 




## Setup der Studie

### Forschungsfragen

Wie gut ist die Interpretationsfähigkeit von LLMs im bezug zu heterogenen Datensätzen.



### Datenbasis und Vorbereitung



### Vorgehensweise zur Korrelationsanalyse mit LLM´s



## Benchmark-Metriken und Analysea



## Benchmark-Analyse

Meine Überlegung ist es das Ergebnisse von [@bach2024heterogeneity].

# Ergebnisse



## Darstellung der Ergebnisse





## Vergleich der Ergebnisse



# Diskussion



## Diskussion der Ergebnisse




## Stärken und Limitationen der Arbeit





## Praktische Anwendungsmöglichkeit








# Eidesstattliche Erklärung
\section*{Eidesstattliche Erklärung}
\addcontentsline{toc}{section}{Eidesstattliche Erklärung}

"Ich versichere, dass ich die vorstehende Arbeit selbstständig und
ohne fremde Hilfe angefertigt und mich anderer als der im beigefügten
Verzeichnis angegebenen Hilfmittel nicht bedient habe. Alle Stellen, die
wörtlich oder sinngemäß aus Veröffentlichungen übernommen wurden, sind als
solche kenntlich gemacht. Alle Internetquellen sind der Arbeit beigefügt.
Des Weiteren versichere ich, dass ich die Arbeit voher nicht in einem anderen
Prüfungsverfahren eingereicht habe und dass die eingereichte schriftliche Fassung
der auf dem elektronischen Speichermedium entspricht." ~\\[20mm]
Ort, Datum\hspace{4cm} Unterschrift


# References {-}

